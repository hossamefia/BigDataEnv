{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üóÑÔ∏è Hadoop HDFS Basics\n",
    "\n",
    "This notebook demonstrates basic HDFS operations in the Big Data environment.\n",
    "\n",
    "## Learning Objectives\n",
    "- Connect to HDFS\n",
    "- Perform basic file operations\n",
    "- Upload and download files\n",
    "- Explore HDFS directory structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import subprocess\n",
    "import pandas as pd\n",
    "from hdfs import InsecureClient\n",
    "\n",
    "# HDFS Configuration\n",
    "HDFS_URL = 'http://namenode:9870'\n",
    "HDFS_USER = 'root'\n",
    "\n",
    "print('üöÄ Big Data Environment - Hadoop HDFS Basics')\n",
    "print('=' * 50)\n",
    "print(f'HDFS URL: {HDFS_URL}')\n",
    "print(f'User: {HDFS_USER}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Connect to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HDFS client\n",
    "try:\n",
    "    client = InsecureClient(HDFS_URL, user=HDFS_USER)\n",
    "    print('‚úÖ Successfully connected to HDFS')\n",
    "    \n",
    "    # Test connection by listing root directory\n",
    "    root_files = client.list('/')\n",
    "    print(f'üìÅ Root directory contains: {root_files}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Failed to connect to HDFS: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Basic HDFS Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create directories\n",
    "print('üìÅ Creating HDFS directories...')\n",
    "\n",
    "directories = [\n",
    "    '/user/demo',\n",
    "    '/user/demo/input',\n",
    "    '/user/demo/output',\n",
    "    '/user/demo/processed'\n",
    "]\n",
    "\n",
    "for directory in directories:\n",
    "    try:\n",
    "        client.makedirs(directory)\n",
    "        print(f'‚úÖ Created directory: {directory}')\n",
    "    except:\n",
    "        print(f'‚ÑπÔ∏è  Directory already exists: {directory}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# List directory contents\n",
    "print('üìã Listing HDFS directories:')\n",
    "print('\\n/user directory:')\n",
    "try:\n",
    "    user_files = client.list('/user', status=True)\n",
    "    for item in user_files:\n",
    "        file_type = 'DIR' if item[1]['type'] == 'DIRECTORY' else 'FILE'\n",
    "        size = item[1]['length']\n",
    "        print(f'  {file_type:4} {size:>10} {item[0]}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error listing directory: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Upload Files to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Upload sample data files to HDFS\n",
    "print('üì§ Uploading files to HDFS...')\n",
    "\n",
    "local_files = {\n",
    "    '/home/jovyan/data/users.csv': '/user/demo/input/users.csv',\n",
    "    '/home/jovyan/data/transactions.json': '/user/demo/input/transactions.json',\n",
    "    '/home/jovyan/data/logs.txt': '/user/demo/input/logs.txt'\n",
    "}\n",
    "\n",
    "for local_path, hdfs_path in local_files.items():\n",
    "    try:\n",
    "        if os.path.exists(local_path):\n",
    "            client.upload(hdfs_path, local_path, overwrite=True)\n",
    "            print(f'‚úÖ Uploaded: {local_path} ‚Üí {hdfs_path}')\n",
    "        else:\n",
    "            print(f'‚ö†Ô∏è  File not found: {local_path}')\n",
    "    except Exception as e:\n",
    "        print(f'‚ùå Error uploading {local_path}: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Verify uploaded files\n",
    "print('üîç Verifying uploaded files:')\n",
    "try:\n",
    "    input_files = client.list('/user/demo/input', status=True)\n",
    "    for item in input_files:\n",
    "        file_type = 'DIR' if item[1]['type'] == 'DIRECTORY' else 'FILE'\n",
    "        size = item[1]['length']\n",
    "        modified = item[1]['modificationTime']\n",
    "        print(f'  {file_type:4} {size:>8} bytes {item[0]}')\n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error listing files: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Read Files from HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file from HDFS\n",
    "print('üìñ Reading users.csv from HDFS:')\n",
    "try:\n",
    "    with client.read('/user/demo/input/users.csv') as reader:\n",
    "        users_data = reader.read().decode('utf-8')\n",
    "    \n",
    "    # Display first few lines\n",
    "    lines = users_data.split('\\n')[:6]\n",
    "    for i, line in enumerate(lines):\n",
    "        if line.strip():\n",
    "            print(f'  {i+1:2}: {line}')\n",
    "    print(f'  ... ({len(users_data.split(chr(10)))-1} total lines)')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error reading file: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read and parse CSV using pandas\n",
    "print('üìä Processing CSV data with pandas:')\n",
    "try:\n",
    "    with client.read('/user/demo/input/users.csv') as reader:\n",
    "        df_users = pd.read_csv(reader)\n",
    "    \n",
    "    print(f'‚úÖ Loaded {len(df_users)} users')\n",
    "    print('\\nFirst 5 records:')\n",
    "    print(df_users.head())\n",
    "    \n",
    "    print('\\nData summary:')\n",
    "    print(f'  - Total users: {len(df_users)}')\n",
    "    print(f'  - Countries: {df_users[\"country\"].nunique()}')\n",
    "    print(f'  - Average age: {df_users[\"age\"].mean():.1f} years')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error processing CSV: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. HDFS File Operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a processed file and upload to HDFS\n",
    "print('üîÑ Processing data and saving to HDFS:')\n",
    "try:\n",
    "    # Process the users data\n",
    "    country_summary = df_users.groupby('country').agg({\n",
    "        'user_id': 'count',\n",
    "        'age': 'mean'\n",
    "    }).rename(columns={'user_id': 'user_count', 'age': 'avg_age'})\n",
    "    \n",
    "    # Save processed data to local file first\n",
    "    local_processed_file = '/tmp/country_summary.csv'\n",
    "    country_summary.to_csv(local_processed_file)\n",
    "    \n",
    "    # Upload to HDFS\n",
    "    hdfs_processed_file = '/user/demo/processed/country_summary.csv'\n",
    "    client.upload(hdfs_processed_file, local_processed_file, overwrite=True)\n",
    "    \n",
    "    print('‚úÖ Processed data saved to HDFS')\n",
    "    print('Country Summary:')\n",
    "    print(country_summary)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error processing data: {e}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get file information\n",
    "print('‚ÑπÔ∏è  File information:')\n",
    "try:\n",
    "    files_to_check = [\n",
    "        '/user/demo/input/users.csv',\n",
    "        '/user/demo/input/transactions.json',\n",
    "        '/user/demo/processed/country_summary.csv'\n",
    "    ]\n",
    "    \n",
    "    for file_path in files_to_check:\n",
    "        try:\n",
    "            status = client.status(file_path)\n",
    "            print(f'\\nüìÑ {file_path}:')\n",
    "            print(f'   Size: {status[\"length\"]} bytes')\n",
    "            print(f'   Type: {status[\"type\"]}')\n",
    "            print(f'   Replication: {status[\"replication\"]}')\n",
    "            print(f'   Block Size: {status[\"blockSize\"]} bytes')\n",
    "        except:\n",
    "            print(f'‚ùå File not found: {file_path}')\n",
    "            \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error getting file info: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. HDFS Administration Commands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check HDFS disk usage\n",
    "print('üíæ HDFS Disk Usage:')\n",
    "try:\n",
    "    # Use client.status to get disk usage info\n",
    "    content_summary = client.content('/user/demo')\n",
    "    print(f'Directory: /user/demo')\n",
    "    print(f'  Files: {content_summary[\"fileCount\"]}')\n",
    "    print(f'  Directories: {content_summary[\"directoryCount\"]}')\n",
    "    print(f'  Size: {content_summary[\"length\"]} bytes')\n",
    "    print(f'  Space Consumed: {content_summary[\"spaceConsumed\"]} bytes')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error getting disk usage: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Cleanup (Optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: Clean up created files and directories\n",
    "# Uncomment the following lines if you want to clean up\n",
    "\n",
    "# print('üßπ Cleaning up HDFS files...')\n",
    "# try:\n",
    "#     client.delete('/user/demo', recursive=True)\n",
    "#     print('‚úÖ Cleanup completed')\n",
    "# except Exception as e:\n",
    "#     print(f'‚ùå Error during cleanup: {e}')\n",
    "\n",
    "print('üí° To clean up, uncomment and run the cleanup code above')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **HDFS Connection**: How to connect to HDFS using Python\n",
    "2. **Directory Operations**: Creating and listing directories\n",
    "3. **File Upload**: Uploading local files to HDFS\n",
    "4. **File Reading**: Reading and processing files from HDFS\n",
    "5. **Data Processing**: Processing data and saving results back to HDFS\n",
    "6. **File Management**: Getting file information and disk usage\n",
    "\n",
    "### Next Steps\n",
    "- Explore the **02-spark-intro.ipynb** notebook to learn Spark basics\n",
    "- Check out the Hadoop NameNode UI at http://localhost:9870\n",
    "- Browse HDFS files through the web interface\n",
    "\n",
    "### üîó Useful Links\n",
    "- **NameNode UI**: http://localhost:9870\n",
    "- **DataNode UI**: http://localhost:9864\n",
    "- **HDFS Documentation**: https://hadoop.apache.org/docs/stable/hadoop-project-dist/hadoop-hdfs/HdfsUserGuide.html"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}