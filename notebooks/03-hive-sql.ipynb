{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hive SQL Operations\n",
    "\n",
    "This notebook demonstrates how to use Hive SQL with Spark for data warehousing operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Hive-SQL-Operations\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Catalog Implementation: {spark.conf.get('spark.sql.catalogImplementation')}\")\n",
    "print(f\"Warehouse Directory: {spark.conf.get('spark.sql.warehouse.dir')}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check existing databases\n",
    "print(\"Existing databases:\")\n",
    "spark.sql(\"SHOW DATABASES\").show()\n",
    "\n",
    "# Create a new database for our examples\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bigdata_demo\")\n",
    "spark.sql(\"USE bigdata_demo\")\n",
    "print(\"\\nUsing database: bigdata_demo\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample customer data\n",
    "customer_data = [\n",
    "    (1, \"John Doe\", \"john@email.com\", \"New York\", \"Premium\"),\n",
    "    (2, \"Jane Smith\", \"jane@email.com\", \"California\", \"Standard\"),\n",
    "    (3, \"Bob Johnson\", \"bob@email.com\", \"Texas\", \"Premium\"),\n",
    "    (4, \"Alice Brown\", \"alice@email.com\", \"Florida\", \"Basic\"),\n",
    "    (5, \"Charlie Wilson\", \"charlie@email.com\", \"New York\", \"Standard\")\n",
    "]\n",
    "\n",
    "customer_schema = StructType([\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"name\", StringType(), True),\n",
    "    StructField(\"email\", StringType(), True),\n",
    "    StructField(\"state\", StringType(), True),\n",
    "    StructField(\"membership\", StringType(), True)\n",
    "])\n",
    "\n",
    "customer_df = spark.createDataFrame(customer_data, customer_schema)\n",
    "print(\"Customer data created:\")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample order data\n",
    "order_data = [\n",
    "    (101, 1, \"2023-01-15\", 250.00, \"Electronics\"),\n",
    "    (102, 2, \"2023-01-16\", 120.00, \"Books\"),\n",
    "    (103, 1, \"2023-01-17\", 75.00, \"Clothing\"),\n",
    "    (104, 3, \"2023-01-18\", 400.00, \"Electronics\"),\n",
    "    (105, 4, \"2023-01-19\", 30.00, \"Books\"),\n",
    "    (106, 2, \"2023-01-20\", 200.00, \"Electronics\"),\n",
    "    (107, 5, \"2023-01-21\", 90.00, \"Clothing\"),\n",
    "    (108, 3, \"2023-01-22\", 180.00, \"Books\"),\n",
    "    (109, 1, \"2023-01-23\", 320.00, \"Electronics\"),\n",
    "    (110, 4, \"2023-01-24\", 45.00, \"Clothing\")\n",
    "]\n",
    "\n",
    "order_schema = StructType([\n",
    "    StructField(\"order_id\", IntegerType(), True),\n",
    "    StructField(\"customer_id\", IntegerType(), True),\n",
    "    StructField(\"order_date\", StringType(), True),\n",
    "    StructField(\"amount\", DoubleType(), True),\n",
    "    StructField(\"category\", StringType(), True)\n",
    "])\n",
    "\n",
    "order_df = spark.createDataFrame(order_data, order_schema)\n",
    "print(\"Order data created:\")\n",
    "order_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive tables\n",
    "# Create customers table\n",
    "customer_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/user/hive/warehouse/bigdata_demo.db/customers\") \\\n",
    "    .saveAsTable(\"customers\")\n",
    "\n",
    "# Create orders table\n",
    "order_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/user/hive/warehouse/bigdata_demo.db/orders\") \\\n",
    "    .saveAsTable(\"orders\")\n",
    "\n",
    "print(\"Hive tables created successfully!\")\n",
    "\n",
    "# Show tables in the database\n",
    "print(\"\\nTables in bigdata_demo database:\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SQL queries\n",
    "print(\"All customers:\")\n",
    "spark.sql(\"SELECT * FROM customers\").show()\n",
    "\n",
    "print(\"\\nAll orders:\")\n",
    "spark.sql(\"SELECT * FROM orders\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL queries\n",
    "print(\"Customer order summary:\")\n",
    "customer_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name,\n",
    "        c.state,\n",
    "        c.membership,\n",
    "        COUNT(o.order_id) as total_orders,\n",
    "        SUM(o.amount) as total_spent,\n",
    "        AVG(o.amount) as avg_order_value\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.name, c.state, c.membership\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_summary.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sales by category and membership\n",
    "print(\"Sales analysis by category and membership:\")\n",
    "category_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.membership,\n",
    "        o.category,\n",
    "        COUNT(*) as order_count,\n",
    "        SUM(o.amount) as total_sales,\n",
    "        AVG(o.amount) as avg_amount\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.membership, o.category\n",
    "    ORDER BY c.membership, total_sales DESC\n",
    "\"\"\")\n",
    "\n",
    "category_analysis.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions in SQL\n",
    "print(\"Customer ranking by total spending:\")\n",
    "customer_ranking = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        c.name,\n",
    "        c.state,\n",
    "        SUM(o.amount) as total_spent,\n",
    "        RANK() OVER (ORDER BY SUM(o.amount) DESC) as spending_rank,\n",
    "        RANK() OVER (PARTITION BY c.state ORDER BY SUM(o.amount) DESC) as state_rank\n",
    "    FROM customers c\n",
    "    JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.name, c.state\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "\n",
    "customer_ranking.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a partitioned table for better performance\n",
    "print(\"Creating partitioned table by category:\")\n",
    "\n",
    "# First, create the data with proper partitioning\n",
    "order_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .option(\"path\", \"hdfs://namenode:9000/user/hive/warehouse/bigdata_demo.db/orders_partitioned\") \\\n",
    "    .saveAsTable(\"orders_partitioned\")\n",
    "\n",
    "print(\"Partitioned table created!\")\n",
    "\n",
    "# Query the partitioned table\n",
    "print(\"\\nQuerying Electronics orders from partitioned table:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT order_id, customer_id, order_date, amount \n",
    "    FROM orders_partitioned \n",
    "    WHERE category = 'Electronics'\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a view for complex queries\n",
    "spark.sql(\"\"\"\n",
    "    CREATE OR REPLACE TEMPORARY VIEW customer_metrics AS\n",
    "    SELECT \n",
    "        c.customer_id,\n",
    "        c.name,\n",
    "        c.state,\n",
    "        c.membership,\n",
    "        COUNT(o.order_id) as order_count,\n",
    "        SUM(o.amount) as total_spent,\n",
    "        AVG(o.amount) as avg_order_value,\n",
    "        MIN(o.order_date) as first_order,\n",
    "        MAX(o.order_date) as last_order\n",
    "    FROM customers c\n",
    "    LEFT JOIN orders o ON c.customer_id = o.customer_id\n",
    "    GROUP BY c.customer_id, c.name, c.state, c.membership\n",
    "\"\"\")\n",
    "\n",
    "print(\"Customer metrics view created!\")\n",
    "\n",
    "# Use the view\n",
    "print(\"\\nPremium customers with high spending:\")\n",
    "spark.sql(\"\"\"\n",
    "    SELECT name, state, total_spent, order_count\n",
    "    FROM customer_metrics\n",
    "    WHERE membership = 'Premium' AND total_spent > 300\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export results to HDFS\n",
    "print(\"Exporting customer summary to HDFS...\")\n",
    "customer_summary.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/data/customer_summary\")\n",
    "\n",
    "print(\"Export completed!\")\n",
    "\n",
    "# Verify the export\n",
    "exported_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/data/customer_summary\")\n",
    "\n",
    "print(\"\\nVerifying exported data:\")\n",
    "exported_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table information\n",
    "print(\"Customers table information:\")\n",
    "spark.sql(\"DESCRIBE FORMATTED customers\").show(50, truncate=False)\n",
    "\n",
    "print(\"\\nPartitioned orders table information:\")\n",
    "spark.sql(\"SHOW PARTITIONS orders_partitioned\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "print(\"Session information:\")\n",
    "print(f\"Current database: {spark.sql('SELECT current_database()').collect()[0][0]}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}