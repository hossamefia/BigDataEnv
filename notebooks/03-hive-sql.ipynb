{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üîç Apache Hive SQL Operations\n",
    "\n",
    "This notebook demonstrates Hive SQL operations with PostgreSQL metastore integration.\n",
    "\n",
    "## Learning Objectives\n",
    "- Connect to Hive with PostgreSQL metastore\n",
    "- Create databases and tables\n",
    "- Load data into Hive tables\n",
    "- Perform SQL queries and analytics\n",
    "- Work with partitioned tables"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Hive Setup and Connection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "\n",
    "print('üöÄ Big Data Environment - Apache Hive SQL Operations')\n",
    "print('=' * 60)\n",
    "\n",
    "# Create Spark session with Hive support\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataEnv-HiveSQL\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print('‚úÖ Spark Session with Hive support created')\n",
    "print(f'üóÑÔ∏è  Warehouse Directory: hdfs://namenode:9000/user/hive/warehouse')\n",
    "print(f'üîó Metastore URI: thrift://hive-metastore:9083')\n",
    "\n",
    "# Test Hive connection\n",
    "try:\n",
    "    databases = spark.sql(\"SHOW DATABASES\")\n",
    "    print('\\nüìã Available Databases:')\n",
    "    databases.show()\nexcept Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Hive connection issue: {e}')\n",
    "    print('üí° Continuing with Spark SQL (Hive compatibility mode)')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Create Database and Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a new database\n",
    "print('üèóÔ∏è  Creating Hive Database and Tables:')\n",
    "\n",
    "try:\n",
    "    # Create database\n",
    "    spark.sql(\"CREATE DATABASE IF NOT EXISTS bigdata_demo\")\n",
    "    spark.sql(\"USE bigdata_demo\")\n",
    "    print('‚úÖ Database \"bigdata_demo\" created and selected')\n",
    "    \n",
    "    # Show current database\n",
    "    current_db = spark.sql(\"SELECT current_database()\")\n",
    "    current_db.show()\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'‚ùå Error creating database: {e}')\n",
    "    print('üí° Using default database')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create users table\n",
    "print('üë• Creating Users Table:')\n",
    "\n",
    "create_users_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS users (\n",
    "    user_id INT,\n",
    "    name STRING,\n",
    "    email STRING,\n",
    "    age INT,\n",
    "    city STRING,\n",
    "    country STRING\n",
    ")\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://namenode:9000/user/hive/warehouse/bigdata_demo.db/users'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(create_users_table)\n",
    "    print('‚úÖ Users table created successfully')\nexcept Exception as e:\n",
    "    print(f'‚ùå Error creating users table: {e}')\n",
    "\n",
    "# Create transactions table\n",
    "print('\\nüí≥ Creating Transactions Table:')\n",
    "\n",
    "create_transactions_table = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS transactions (\n",
    "    transaction_id STRING,\n",
    "    user_id INT,\n",
    "    amount DOUBLE,\n",
    "    currency STRING,\n",
    "    merchant STRING,\n",
    "    category STRING,\n",
    "    status STRING,\n",
    "    transaction_date DATE\n",
    ")\n",
    "PARTITIONED BY (year INT, month INT)\n",
    "STORED AS PARQUET\n",
    "LOCATION 'hdfs://namenode:9000/user/hive/warehouse/bigdata_demo.db/transactions'\n",
    "\"\"\"\n",
    "\n",
    "try:\n",
    "    spark.sql(create_transactions_table)\n",
    "    print('‚úÖ Transactions table created successfully (partitioned)')\nexcept Exception as e:\n",
    "    print(f'‚ùå Error creating transactions table: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Load Data into Hive Tables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load users data from HDFS CSV file\n",
    "print('üì§ Loading Users Data:')\n",
    "\n",
    "try:\n",
    "    # Read users CSV from HDFS\n",
    "    users_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"hdfs://namenode:9000/user/demo/input/users.csv\")\n",
    "    \n",
    "    # Insert into Hive table\n",
    "    users_df.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .insertInto(\"users\")\n",
    "    \n",
    "    print('‚úÖ Users data loaded into Hive table')\n",
    "    \n",
    "    # Verify data\n",
    "    user_count = spark.sql(\"SELECT COUNT(*) as count FROM users\")\n",
    "    user_count.show()\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ùå Error loading users data: {e}')\n",
    "    # Create sample data if file loading fails\n",
    "    sample_users = [\n",
    "        (1, \"John Smith\", \"john@email.com\", 28, \"New York\", \"USA\"),\n",
    "        (2, \"Emma Johnson\", \"emma@email.com\", 34, \"London\", \"UK\"),\n",
    "        (3, \"Michael Chen\", \"michael@email.com\", 22, \"Toronto\", \"Canada\"),\n",
    "        (4, \"Sarah Williams\", \"sarah@email.com\", 31, \"Sydney\", \"Australia\"),\n",
    "        (5, \"David Brown\", \"david@email.com\", 45, \"Berlin\", \"Germany\")\n",
    "    ]\n",
    "    columns = [\"user_id\", \"name\", \"email\", \"age\", \"city\", \"country\"]\n",
    "    sample_df = spark.createDataFrame(sample_users, columns)\n",
    "    sample_df.createOrReplaceTempView(\"users\")\n",
    "    print('‚ÑπÔ∏è  Using sample users data in temporary view')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and process transactions data\n",
    "print('üí≥ Loading Transactions Data:')\n",
    "\n",
    "try:\n",
    "    # Read transactions JSON from HDFS\n",
    "    transactions_raw = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .json(\"hdfs://namenode:9000/user/demo/input/transactions.json\")\n",
    "    \n",
    "    # Process and add partitioning columns\n",
    "    transactions_processed = transactions_raw \\\n",
    "        .withColumn(\"transaction_date\", to_date(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"year\", year(to_date(col(\"timestamp\")))) \\\n",
    "        .withColumn(\"month\", month(to_date(col(\"timestamp\")))) \\\n",
    "        .select(\n",
    "            \"transaction_id\", \"user_id\", \"amount\", \"currency\", \n",
    "            \"merchant\", \"category\", \"status\", \"transaction_date\", \n",
    "            \"year\", \"month\"\n",
    "        )\n",
    "    \n",
    "    # Show sample processed data\n",
    "    print('\\nüîç Sample Processed Transactions:')\n",
    "    transactions_processed.show(5)\n",
    "    \n",
    "    # Create temporary view for SQL operations\n",
    "    transactions_processed.createOrReplaceTempView(\"transactions\")\n",
    "    print('‚úÖ Transactions data processed and available for queries')\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ùå Error loading transactions: {e}')\n",
    "    # Create sample transactions data\n",
    "    sample_transactions = [\n",
    "        (\"TXN001\", 1, 150.50, \"USD\", \"Amazon\", \"Electronics\", \"completed\", \"2024-01-15\", 2024, 1),\n",
    "        (\"TXN002\", 2, 89.99, \"GBP\", \"Tesco\", \"Groceries\", \"completed\", \"2024-01-15\", 2024, 1),\n",
    "        (\"TXN003\", 3, 299.00, \"CAD\", \"Best Buy\", \"Electronics\", \"pending\", \"2024-01-15\", 2024, 1),\n",
    "        (\"TXN004\", 1, 45.75, \"USD\", \"Starbucks\", \"Food & Drink\", \"completed\", \"2024-01-16\", 2024, 1),\n",
    "        (\"TXN005\", 4, 120.00, \"AUD\", \"Woolworths\", \"Groceries\", \"completed\", \"2024-01-16\", 2024, 1)\n",
    "    ]\n",
    "    columns = [\"transaction_id\", \"user_id\", \"amount\", \"currency\", \"merchant\", \"category\", \"status\", \"transaction_date\", \"year\", \"month\"]\n",
    "    sample_trans_df = spark.createDataFrame(sample_transactions, columns)\n",
    "    sample_trans_df.createOrReplaceTempView(\"transactions\")\n",
    "    print('‚ÑπÔ∏è  Using sample transactions data in temporary view')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Basic Hive SQL Queries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic SELECT queries\n",
    "print('üîç Basic Hive SQL Queries:')\n",
    "\n",
    "# Query 1: Show all users\n",
    "print('\\nüë• All Users:')\n",
    "all_users = spark.sql(\"\"\"\n",
    "    SELECT user_id, name, email, age, city, country \n",
    "    FROM users \n",
    "    ORDER BY user_id\n",
    "\"\"\")\n",
    "all_users.show()\n",
    "\n",
    "# Query 2: Users by country with statistics\n",
    "print('\\nüåç Users by Country:')\n",
    "users_by_country = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        country,\n",
    "        COUNT(*) as user_count,\n",
    "        AVG(age) as avg_age,\n",
    "        MIN(age) as min_age,\n",
    "        MAX(age) as max_age\n",
    "    FROM users \n",
    "    GROUP BY country \n",
    "    ORDER BY user_count DESC\n",
    "\"\"\")\n",
    "users_by_country.show()\n",
    "\n",
    "# Query 3: Users in specific age range\n",
    "print('\\nüéØ Users between 25 and 35:')\n",
    "age_range_users = spark.sql(\"\"\"\n",
    "    SELECT name, age, city, country \n",
    "    FROM users \n",
    "    WHERE age BETWEEN 25 AND 35 \n",
    "    ORDER BY age\n",
    "\"\"\")\n",
    "age_range_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Transaction analysis queries\n",
    "print('üí∞ Transaction Analysis Queries:')\n",
    "\n",
    "# Query 1: Transaction summary by status\n",
    "print('\\nüìä Transactions by Status:')\n",
    "transactions_by_status = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        status,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_amount,\n",
    "        AVG(amount) as avg_amount\n",
    "    FROM transactions \n",
    "    GROUP BY status \n",
    "    ORDER BY transaction_count DESC\n",
    "\"\"\")\n",
    "transactions_by_status.show()\n",
    "\n",
    "# Query 2: Top merchants by revenue\n",
    "print('\\nüè™ Top Merchants by Revenue:')\n",
    "top_merchants = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        merchant,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_revenue,\n",
    "        AVG(amount) as avg_transaction_value\n",
    "    FROM transactions \n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY merchant \n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "top_merchants.show()\n",
    "\n",
    "# Query 3: Transactions by category\n",
    "print('\\nüì¶ Transactions by Category:')\n",
    "category_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_spent,\n",
    "        ROUND(AVG(amount), 2) as avg_amount\n",
    "    FROM transactions \n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY category \n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "category_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced Hive SQL - JOINs and Subqueries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complex JOIN queries\n",
    "print('üîó Advanced JOIN Queries:')\n",
    "\n",
    "# Query 1: User spending analysis with JOIN\n",
    "print('\\nüí∏ User Spending Analysis:')\n",
    "user_spending = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name,\n",
    "        u.country,\n",
    "        u.age,\n",
    "        COUNT(t.transaction_id) as total_transactions,\n",
    "        SUM(CASE WHEN t.status = 'completed' THEN t.amount ELSE 0 END) as total_spent,\n",
    "        SUM(CASE WHEN t.status = 'pending' THEN t.amount ELSE 0 END) as pending_amount,\n",
    "        SUM(CASE WHEN t.status = 'failed' THEN t.amount ELSE 0 END) as failed_amount,\n",
    "        ROUND(AVG(t.amount), 2) as avg_transaction_amount\n",
    "    FROM users u\n",
    "    LEFT JOIN transactions t ON u.user_id = t.user_id\n",
    "    GROUP BY u.user_id, u.name, u.country, u.age\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "user_spending.show()\n",
    "\n",
    "# Query 2: Country-wise spending patterns\n",
    "print('\\nüåç Country-wise Spending Patterns:')\n",
    "country_spending = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.country,\n",
    "        COUNT(DISTINCT u.user_id) as active_users,\n",
    "        COUNT(t.transaction_id) as total_transactions,\n",
    "        SUM(t.amount) as total_revenue,\n",
    "        ROUND(AVG(t.amount), 2) as avg_transaction_value,\n",
    "        ROUND(SUM(t.amount) / COUNT(DISTINCT u.user_id), 2) as revenue_per_user\n",
    "    FROM users u\n",
    "    JOIN transactions t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.country\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "country_spending.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analytics with window functions\n",
    "print('üìä Advanced Analytics with Window Functions:')\n",
    "\n",
    "# Query 1: User ranking by spending\n",
    "print('\\nüèÜ User Ranking by Total Spending:')\n",
    "user_ranking = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.name,\n",
    "        u.country,\n",
    "        SUM(t.amount) as total_spent,\n",
    "        RANK() OVER (ORDER BY SUM(t.amount) DESC) as spending_rank,\n",
    "        DENSE_RANK() OVER (PARTITION BY u.country ORDER BY SUM(t.amount) DESC) as country_rank\n",
    "    FROM users u\n",
    "    JOIN transactions t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.user_id, u.name, u.country\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "user_ranking.show()\n",
    "\n",
    "# Query 2: Running totals and percentages\n",
    "print('\\nüìà Running Totals and Cumulative Analysis:')\n",
    "running_totals = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        merchant,\n",
    "        SUM(amount) as merchant_revenue,\n",
    "        SUM(SUM(amount)) OVER (ORDER BY SUM(amount) DESC \n",
    "                              ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_total,\n",
    "        ROUND(SUM(amount) * 100.0 / SUM(SUM(amount)) OVER (), 2) as pct_of_total\n",
    "    FROM transactions \n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY merchant\n",
    "    ORDER BY merchant_revenue DESC\n",
    "\"\"\")\n",
    "running_totals.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export and Table Management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create summary tables\n",
    "print('üíæ Creating Summary Tables:')\n",
    "\n",
    "# Create user summary table\n",
    "print('\\nüë• Creating User Summary Table:')\n",
    "create_user_summary = spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS user_summary AS\n",
    "    SELECT \n",
    "        u.user_id,\n",
    "        u.name,\n",
    "        u.country,\n",
    "        u.age,\n",
    "        COUNT(t.transaction_id) as total_transactions,\n",
    "        SUM(CASE WHEN t.status = 'completed' THEN t.amount ELSE 0 END) as total_spent,\n",
    "        ROUND(AVG(CASE WHEN t.status = 'completed' THEN t.amount END), 2) as avg_transaction\n",
    "    FROM users u\n",
    "    LEFT JOIN transactions t ON u.user_id = t.user_id\n",
    "    GROUP BY u.user_id, u.name, u.country, u.age\n",
    "\"\"\")\n",
    "\n",
    "print('‚úÖ User summary table created')\n",
    "\n",
    "# Show the summary table\n",
    "print('\\nüìä User Summary Table Contents:')\n",
    "spark.sql(\"SELECT * FROM user_summary ORDER BY total_spent DESC\").show()\n",
    "\n",
    "# Create merchant performance table\n",
    "print('\\nüè™ Creating Merchant Performance Table:')\n",
    "merchant_performance = spark.sql(\"\"\"\n",
    "    CREATE TABLE IF NOT EXISTS merchant_performance AS\n",
    "    SELECT \n",
    "        merchant,\n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_revenue,\n",
    "        ROUND(AVG(amount), 2) as avg_transaction_value,\n",
    "        COUNT(CASE WHEN status = 'completed' THEN 1 END) as successful_transactions,\n",
    "        COUNT(CASE WHEN status = 'failed' THEN 1 END) as failed_transactions,\n",
    "        ROUND(COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*), 2) as success_rate\n",
    "    FROM transactions\n",
    "    GROUP BY merchant, category\n",
    "\"\"\")\n",
    "\n",
    "print('‚úÖ Merchant performance table created')\n",
    "print('\\nüìä Merchant Performance:')\n",
    "spark.sql(\"SELECT * FROM merchant_performance ORDER BY total_revenue DESC\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export data to different formats\n",
    "print('üì§ Exporting Data to HDFS:')\n",
    "\n",
    "try:\n",
    "    # Export user summary as Parquet\n",
    "    user_summary_df = spark.sql(\"SELECT * FROM user_summary\")\n",
    "    user_summary_df.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://namenode:9000/user/demo/output/hive_user_summary\")\n",
    "    print('‚úÖ User summary exported as Parquet')\n",
    "    \n",
    "    # Export merchant performance as JSON\n",
    "    merchant_perf_df = spark.sql(\"SELECT * FROM merchant_performance\")\n",
    "    merchant_perf_df.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .json(\"hdfs://namenode:9000/user/demo/output/hive_merchant_performance\")\n",
    "    print('‚úÖ Merchant performance exported as JSON')\n",
    "    \n",
    "    # Export detailed analysis as CSV\n",
    "    detailed_analysis = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            u.name,\n",
    "            u.country,\n",
    "            t.merchant,\n",
    "            t.category,\n",
    "            t.amount,\n",
    "            t.currency,\n",
    "            t.status,\n",
    "            t.transaction_date\n",
    "        FROM users u\n",
    "        JOIN transactions t ON u.user_id = t.user_id\n",
    "        ORDER BY t.transaction_date, u.name\n",
    "    \"\"\")\n",
    "    \n",
    "    detailed_analysis.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"hdfs://namenode:9000/user/demo/output/hive_detailed_analysis\")\n",
    "    print('‚úÖ Detailed analysis exported as CSV')\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ùå Error exporting data: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Table Information and Metadata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show table information\n",
    "print('‚ÑπÔ∏è  Table Information and Metadata:')\n",
    "\n",
    "# List all tables\n",
    "print('\\nüìã Available Tables:')\n",
    "try:\n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    tables.show()\nexcept:\n",
    "    print('üìä Temporary views:')\n",
    "    spark.catalog.listTables().show()\n",
    "\n",
    "# Describe table structure\n",
    "print('\\nüîç Users Table Structure:')\n",
    "try:\n",
    "    spark.sql(\"DESCRIBE EXTENDED users\").show(20, False)\nexcept:\n",
    "    spark.sql(\"DESCRIBE users\").show()\n",
    "\n",
    "# Show table statistics\n",
    "print('\\nüìä Table Statistics:')\n",
    "try:\n",
    "    # Analyze table to compute statistics\n",
    "    spark.sql(\"ANALYZE TABLE users COMPUTE STATISTICS\")\n",
    "    spark.sql(\"SHOW TBLPROPERTIES users\").show(10, False)\nexcept Exception as e:\n",
    "    print(f'‚ÑπÔ∏è  Statistics not available: {e}')\n",
    "    \n",
    "# Show table size and location\n",
    "print('\\nüíæ Table Storage Information:')\n",
    "users_count = spark.sql(\"SELECT COUNT(*) as total_users FROM users\")\n",
    "transactions_count = spark.sql(\"SELECT COUNT(*) as total_transactions FROM transactions\")\n",
    "\n",
    "print('Table sizes:')\n",
    "users_count.show()\n",
    "transactions_count.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Performance Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization techniques\n",
    "print('‚ö° Performance Optimization:')\n",
    "\n",
    "# Query with explain plan\n",
    "print('\\nüîç Query Execution Plan:')\n",
    "complex_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.country,\n",
    "        COUNT(*) as user_count,\n",
    "        SUM(t.amount) as total_revenue\n",
    "    FROM users u\n",
    "    JOIN transactions t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.country\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "# Show execution plan\n",
    "complex_query.explain(True)\n",
    "\n",
    "# Cache frequently used table\n",
    "print('\\nüíæ Caching Frequently Used Tables:')\n",
    "spark.sql(\"CACHE TABLE users\")\n",
    "print('‚úÖ Users table cached in memory')\n",
    "\n",
    "# Show cached tables\n",
    "try:\n",
    "    cached_tables = spark.sql(\"SHOW TABLES\")\n",
    "    print('\\nüìã Cached Tables:')\n",
    "    cached_tables.show()\nexcept:\n",
    "    print('üí° Cache information not available via SQL')\n",
    "\n",
    "print('\\nüí° Performance Tips:')\n",
    "print('  - Use partitioned tables for large datasets')\n",
    "print('  - Cache frequently accessed tables')\n",
    "print('  - Use appropriate file formats (Parquet, ORC)')\n",
    "print('  - Optimize JOIN order and conditions')\n",
    "print('  - Use columnar storage for analytics workloads')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Cleanup and Best Practices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleanup operations\n",
    "print('üßπ Cleanup and Best Practices:')\n",
    "\n",
    "# Uncache tables\n",
    "print('\\nüíæ Uncaching Tables:')\n",
    "try:\n",
    "    spark.sql(\"UNCACHE TABLE users\")\n",
    "    print('‚úÖ Users table removed from cache')\nexcept:\n",
    "    print('‚ÑπÔ∏è  No cached tables to remove')\n",
    "\n",
    "# Show current database and tables\n",
    "print('\\nüìã Current Database Status:')\n",
    "try:\n",
    "    current_db = spark.sql(\"SELECT current_database()\")\n",
    "    current_db.show()\n",
    "    \n",
    "    tables = spark.sql(\"SHOW TABLES\")\n",
    "    print('\\nTables in current database:')\n",
    "    tables.show()\nexcept:\n",
    "    print('‚ÑπÔ∏è  Database information not available')\n",
    "\n",
    "print('\\nüí° Hive Best Practices:')\n",
    "best_practices = [\n",
    "    \"1. Use appropriate data types for better performance\",\n",
    "    \"2. Partition tables based on query patterns\",\n",
    "    \"3. Use bucketing for frequently joined columns\",\n",
    "    \"4. Choose optimal file formats (Parquet for analytics)\",\n",
    "    \"5. Regularly analyze tables to update statistics\",\n",
    "    \"6. Use external tables for data flexibility\",\n",
    "    \"7. Implement proper data lifecycle management\",\n",
    "    \"8. Monitor and optimize query performance\"\n",
    "]\n",
    "\n",
    "for practice in best_practices:\n",
    "    print(f'   {practice}')\n",
    "\n",
    "print('\\nüîó Useful Resources:')\n",
    "print('   - HiveServer2 UI: http://localhost:10002')\n",
    "print('   - Spark SQL UI: http://localhost:4040')\n",
    "print('   - HDFS NameNode: http://localhost:9870')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## üéØ Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Hive Setup**: Connecting to Hive with PostgreSQL metastore\n",
    "2. **Database Management**: Creating databases and tables with proper schemas\n",
    "3. **Data Loading**: Loading data from various sources into Hive tables\n",
    "4. **SQL Queries**: Basic and advanced SQL operations in Hive\n",
    "5. **JOINs and Analytics**: Complex queries with joins and window functions\n",
    "6. **Data Export**: Exporting results to different formats and locations\n",
    "7. **Metadata Management**: Working with table information and statistics\n",
    "8. **Performance Optimization**: Query optimization and caching strategies\n",
    "\n",
    "### Key Hive Concepts Covered\n",
    "- **DDL Operations**: `CREATE DATABASE`, `CREATE TABLE`, `DESCRIBE`\n",
    "- **DML Operations**: `INSERT`, `SELECT`, `JOIN`, `GROUP BY`\n",
    "- **Analytics Functions**: `RANK()`, `DENSE_RANK()`, Window functions\n",
    "- **Data Formats**: Parquet, JSON, CSV storage options\n",
    "- **Partitioning**: Table partitioning for better performance\n",
    "\n",
    "### Integration Benefits\n",
    "- **PostgreSQL Metastore**: Reliable metadata storage (no Derby issues)\n",
    "- **Spark Integration**: Fast query execution with Spark engine\n",
    "- **HDFS Storage**: Scalable distributed file system\n",
    "- **Schema Evolution**: Flexible schema management\n",
    "\n",
    "### Next Steps\n",
    "- Explore the **04-integration.ipynb** notebook for full stack integration\n",
    "- Check out the HiveServer2 UI at http://localhost:10002\n",
    "- Browse HDFS files at http://localhost:9870\n",
    "\n",
    "### üîó Useful Links\n",
    "- **HiveServer2 UI**: http://localhost:10002\n",
    "- **Spark Master UI**: http://localhost:8080\n",
    "- **HDFS NameNode UI**: http://localhost:9870\n",
    "- **Hive Documentation**: https://hive.apache.org/\n",
    "- **HiveQL Reference**: https://cwiki.apache.org/confluence/display/Hive/LanguageManual"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}