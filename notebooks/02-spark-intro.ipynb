{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Spark Introduction\n",
    "\n",
    "This notebook introduces Apache Spark fundamentals including DataFrames, RDDs, and basic operations.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print(\"Libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Spark Session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Spark-Introduction\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Spark Context: {spark.sparkContext}\")\n",
    "print(f\"Web UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create sample data\n",
    "sales_data = [\n",
    "    (1, \"2023-01-01\", \"Electronics\", \"Laptop\", 1200.00, 2),\n",
    "    (2, \"2023-01-02\", \"Electronics\", \"Phone\", 800.00, 1),\n",
    "    (3, \"2023-01-03\", \"Clothing\", \"Shirt\", 50.00, 3),\n",
    "    (4, \"2023-01-04\", \"Electronics\", \"Tablet\", 400.00, 1),\n",
    "    (5, \"2023-01-05\", \"Books\", \"Python Guide\", 30.00, 2),\n",
    "    (6, \"2023-01-06\", \"Clothing\", \"Jeans\", 80.00, 1),\n",
    "    (7, \"2023-01-07\", \"Electronics\", \"Mouse\", 25.00, 4),\n",
    "    (8, \"2023-01-08\", \"Books\", \"Data Science\", 45.00, 1),\n",
    "    (9, \"2023-01-09\", \"Electronics\", \"Keyboard\", 60.00, 2),\n",
    "    (10, \"2023-01-10\", \"Clothing\", \"Shoes\", 120.00, 1)\n",
    "]\n",
    "\n",
    "schema = StructType([\n",
    "    StructField(\"id\", IntegerType(), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "    StructField(\"category\", StringType(), True),\n",
    "    StructField(\"product\", StringType(), True),\n",
    "    StructField(\"price\", DoubleType(), True),\n",
    "    StructField(\"quantity\", IntegerType(), True)\n",
    "])\n",
    "\n",
    "df = spark.createDataFrame(sales_data, schema)\n",
    "print(\"Sample sales data created:\")\n",
    "df.show()\n",
    "df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame operations\n",
    "print(\"DataFrame Info:\")\n",
    "print(f\"Number of rows: {df.count()}\")\n",
    "print(f\"Number of columns: {len(df.columns)}\")\n",
    "print(f\"Columns: {df.columns}\")\n",
    "\n",
    "print(\"\\nDataFrame Statistics:\")\n",
    "df.describe().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data transformations\n",
    "# Add calculated column for total amount\n",
    "df_with_total = df.withColumn(\"total_amount\", col(\"price\") * col(\"quantity\"))\n",
    "\n",
    "print(\"DataFrame with total amount:\")\n",
    "df_with_total.show()\n",
    "\n",
    "# Filter electronics products\n",
    "electronics_df = df_with_total.filter(col(\"category\") == \"Electronics\")\n",
    "print(\"\\nElectronics products only:\")\n",
    "electronics_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aggregations\n",
    "print(\"Sales by Category:\")\n",
    "category_sales = df_with_total.groupBy(\"category\") \\\n",
    "    .agg(\n",
    "        sum(\"total_amount\").alias(\"total_sales\"),\n",
    "        avg(\"price\").alias(\"avg_price\"),\n",
    "        count(\"*\").alias(\"num_items\")\n",
    "    ) \\\n",
    "    .orderBy(desc(\"total_sales\"))\n",
    "\n",
    "category_sales.show()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "category_pandas = category_sales.toPandas()\n",
    "print(\"\\nCategory sales as Pandas DataFrame:\")\n",
    "print(category_pandas)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualization\n",
    "plt.figure(figsize=(10, 6))\n",
    "\n",
    "# Bar chart of sales by category\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.bar(category_pandas['category'], category_pandas['total_sales'])\n",
    "plt.title('Total Sales by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Sales ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Pie chart of item count by category\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.pie(category_pandas['num_items'], labels=category_pandas['category'], autopct='%1.1f%%')\n",
    "plt.title('Number of Items by Category')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Working with RDDs (lower-level API)\n",
    "rdd = spark.sparkContext.parallelize(range(1, 1001))\n",
    "\n",
    "# Map, filter, reduce operations\n",
    "squared_rdd = rdd.map(lambda x: x ** 2)\n",
    "even_squares = squared_rdd.filter(lambda x: x % 2 == 0)\n",
    "sum_even_squares = even_squares.reduce(lambda a, b: a + b)\n",
    "\n",
    "print(f\"Sum of even squares from 1 to 1000: {sum_even_squares}\")\n",
    "print(f\"First 10 even squares: {even_squares.take(10)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Window functions\n",
    "from pyspark.sql.window import Window\n",
    "\n",
    "# Add row numbers and running totals\n",
    "window_spec = Window.partitionBy(\"category\").orderBy(desc(\"total_amount\"))\n",
    "\n",
    "df_windowed = df_with_total.withColumn(\n",
    "    \"rank_in_category\", \n",
    "    row_number().over(window_spec)\n",
    ").withColumn(\n",
    "    \"running_total\", \n",
    "    sum(\"total_amount\").over(window_spec.rowsBetween(Window.unboundedPreceding, Window.currentRow))\n",
    ")\n",
    "\n",
    "print(\"DataFrame with window functions:\")\n",
    "df_windowed.select(\"category\", \"product\", \"total_amount\", \"rank_in_category\", \"running_total\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save data to HDFS\n",
    "hdfs_path = \"hdfs://namenode:9000/user/data/sales_data\"\n",
    "\n",
    "df_with_total.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(f\"Sales data saved to HDFS: {hdfs_path}\")\n",
    "\n",
    "# Verify the save\n",
    "df_loaded = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(hdfs_path)\n",
    "\n",
    "print(\"\\nData loaded back from HDFS:\")\n",
    "df_loaded.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring\n",
    "print(\"Spark Application Information:\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Application Name: {spark.sparkContext.appName}\")\n",
    "print(f\"Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"Web UI URL: {spark.sparkContext.uiWebUrl}\")\n",
    "\n",
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"\\nSpark session stopped successfully!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}