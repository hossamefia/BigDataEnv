{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ⚡ Apache Spark Introduction\n",
    "\n",
    "This notebook demonstrates Apache Spark fundamentals in the Big Data environment.\n",
    "\n",
    "## Learning Objectives\n",
    "- Initialize Spark session with Hadoop integration\n",
    "- Work with Spark DataFrames and RDDs\n",
    "- Perform data processing and transformations\n",
    "- Save results to HDFS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Spark Session Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import json\n",
    "\n",
    "print('🚀 Big Data Environment - Apache Spark Introduction')\n",
    "print('=' * 55)\n",
    "\n",
    "# Create Spark session with Hadoop integration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataEnv-SparkIntro\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print('✅ Spark Session created successfully')\n",
    "print(f'📊 Spark Version: {spark.version}')\n",
    "print(f'🌐 Spark Master: {spark.sparkContext.master}')\n",
    "print(f'📱 Application ID: {spark.sparkContext.applicationId}')\n",
    "print(f'🔧 Hadoop Version: {spark.sparkContext._jsc.hadoopConfiguration().get(\"hadoop.version\")}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Working with Spark DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read CSV file from HDFS\n",
    "print('📖 Reading users data from HDFS...')\n",
    "\n",
    "try:\n",
    "    # Read users CSV file\n",
    "    users_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"hdfs://namenode:9000/user/demo/input/users.csv\")\n",
    "    \n",
    "    print('✅ Users data loaded successfully')\n",
    "    print(f'📊 Records: {users_df.count()}')\n",
    "    print(f'🔢 Columns: {len(users_df.columns)}')\n",
    "    \n",
    "    # Show schema\n",
    "    print('\\n📋 Schema:')\n",
    "    users_df.printSchema()\n",
    "    \n",
    "    # Show first few records\n",
    "    print('\\n👀 First 5 records:')\n",
    "    users_df.show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Error reading users data: {e}')\n",
    "    # Create sample data if file doesn't exist\n",
    "    sample_data = [\n",
    "        (1, \"John Smith\", \"john@email.com\", 28, \"New York\", \"USA\"),\n",
    "        (2, \"Emma Johnson\", \"emma@email.com\", 34, \"London\", \"UK\"),\n",
    "        (3, \"Michael Chen\", \"michael@email.com\", 22, \"Toronto\", \"Canada\")\n",
    "    ]\n",
    "    schema = [\"user_id\", \"name\", \"email\", \"age\", \"city\", \"country\"]\n",
    "    users_df = spark.createDataFrame(sample_data, schema)\n",
    "    print('ℹ️  Using sample data instead')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Transformations and Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Basic DataFrame operations\n",
    "print('🔍 Data Analysis:')\n",
    "\n",
    "# Count by country\n",
    "print('\\n📊 Users by Country:')\n",
    "country_counts = users_df.groupBy(\"country\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "country_counts.show()\n",
    "\n",
    "# Age statistics\n",
    "print('\\n📈 Age Statistics:')\n",
    "age_stats = users_df.select(\n",
    "    avg(\"age\").alias(\"avg_age\"),\n",
    "    min(\"age\").alias(\"min_age\"),\n",
    "    max(\"age\").alias(\"max_age\"),\n",
    "    stddev(\"age\").alias(\"stddev_age\")\n",
    ")\n",
    "age_stats.show()\n",
    "\n",
    "# Filter users by age\n",
    "print('\\n👥 Users above 30:')\n",
    "mature_users = users_df.filter(col(\"age\") > 30) \\\n",
    "    .select(\"name\", \"age\", \"city\", \"country\")\n",
    "mature_users.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced transformations\n",
    "print('🔄 Advanced Transformations:')\n",
    "\n",
    "# Add age group column\n",
    "users_with_age_group = users_df.withColumn(\n",
    "    \"age_group\",\n",
    "    when(col(\"age\") < 25, \"Young\")\n",
    "    .when(col(\"age\") < 35, \"Adult\")\n",
    "    .otherwise(\"Mature\")\n",
    ")\n",
    "\n",
    "print('\\n👶 Age Group Distribution:')\n",
    "age_group_dist = users_with_age_group.groupBy(\"age_group\") \\\n",
    "    .count() \\\n",
    "    .orderBy(\"age_group\")\n",
    "age_group_dist.show()\n",
    "\n",
    "# Create email domain analysis\n",
    "users_with_domain = users_df.withColumn(\n",
    "    \"email_domain\",\n",
    "    split(col(\"email\"), \"@\").getItem(1)\n",
    ")\n",
    "\n",
    "print('\\n📧 Email Domain Analysis:')\n",
    "domain_analysis = users_with_domain.groupBy(\"email_domain\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "domain_analysis.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Working with JSON Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read JSON data from HDFS\n",
    "print('📖 Reading transactions JSON from HDFS...')\n",
    "\n",
    "try:\n",
    "    # Read transactions JSON file\n",
    "    transactions_df = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .json(\"hdfs://namenode:9000/user/demo/input/transactions.json\")\n",
    "    \n",
    "    print('✅ Transactions data loaded successfully')\n",
    "    print(f'📊 Records: {transactions_df.count()}')\n",
    "    \n",
    "    # Show schema\n",
    "    print('\\n📋 Schema:')\n",
    "    transactions_df.printSchema()\n",
    "    \n",
    "    # Show sample data\n",
    "    print('\\n💳 Sample Transactions:')\n",
    "    transactions_df.select(\"transaction_id\", \"user_id\", \"amount\", \"currency\", \"merchant\", \"status\").show(5)\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Error reading transactions: {e}')\n",
    "    # Create sample transactions data\n",
    "    sample_transactions = [\n",
    "        (\"TXN001\", 1, 150.50, \"USD\", \"Amazon\", \"completed\"),\n",
    "        (\"TXN002\", 2, 89.99, \"GBP\", \"Tesco\", \"completed\"),\n",
    "        (\"TXN003\", 1, 45.75, \"USD\", \"Starbucks\", \"completed\")\n",
    "    ]\n",
    "    schema = [\"transaction_id\", \"user_id\", \"amount\", \"currency\", \"merchant\", \"status\"]\n",
    "    transactions_df = spark.createDataFrame(sample_transactions, schema)\n",
    "    print('ℹ️  Using sample transactions data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Analyze transaction data\n",
    "print('💰 Transaction Analysis:')\n",
    "\n",
    "# Total amount by currency\n",
    "print('\\n💱 Total Amount by Currency:')\n",
    "currency_totals = transactions_df.groupBy(\"currency\") \\\n",
    "    .agg(\n",
    "        sum(\"amount\").alias(\"total_amount\"),\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        avg(\"amount\").alias(\"avg_amount\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"total_amount\").desc())\n",
    "currency_totals.show()\n",
    "\n",
    "# Transaction status distribution\n",
    "print('\\n📊 Transaction Status Distribution:')\n",
    "status_dist = transactions_df.groupBy(\"status\") \\\n",
    "    .count() \\\n",
    "    .orderBy(col(\"count\").desc())\n",
    "status_dist.show()\n",
    "\n",
    "# Top merchants\n",
    "print('\\n🏪 Top Merchants by Transaction Count:')\n",
    "top_merchants = transactions_df.groupBy(\"merchant\") \\\n",
    "    .agg(\n",
    "        count(\"*\").alias(\"transaction_count\"),\n",
    "        sum(\"amount\").alias(\"total_revenue\")\n",
    "    ) \\\n",
    "    .orderBy(col(\"transaction_count\").desc())\n",
    "top_merchants.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Joining DataFrames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Join users and transactions\n",
    "print('🔗 Joining Users and Transactions:')\n",
    "\n",
    "# Join on user_id\n",
    "user_transactions = users_df.join(\n",
    "    transactions_df,\n",
    "    users_df.user_id == transactions_df.user_id,\n",
    "    \"inner\"\n",
    ").select(\n",
    "    users_df.user_id,\n",
    "    users_df.name,\n",
    "    users_df.country,\n",
    "    transactions_df.transaction_id,\n",
    "    transactions_df.amount,\n",
    "    transactions_df.currency,\n",
    "    transactions_df.merchant,\n",
    "    transactions_df.status\n",
    ")\n",
    "\n",
    "print('\\n👥 User Transaction Details:')\n",
    "user_transactions.show(10)\n",
    "\n",
    "# Aggregate by user\n",
    "print('\\n💰 User Spending Summary:')\n",
    "user_spending = user_transactions.groupBy(\n",
    "    \"user_id\", \"name\", \"country\"\n",
    ").agg(\n",
    "    count(\"transaction_id\").alias(\"total_transactions\"),\n",
    "    sum(\"amount\").alias(\"total_spent\"),\n",
    "    avg(\"amount\").alias(\"avg_transaction_amount\")\n",
    ").orderBy(col(\"total_spent\").desc())\n",
    "\n",
    "user_spending.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Working with RDDs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert DataFrame to RDD for low-level operations\n",
    "print('🔧 Working with RDDs:')\n",
    "\n",
    "# Convert users DataFrame to RDD\n",
    "users_rdd = users_df.rdd\n",
    "\n",
    "print(f'📊 RDD Partitions: {users_rdd.getNumPartitions()}')\n",
    "print(f'📊 RDD Count: {users_rdd.count()}')\n",
    "\n",
    "# Map operation: extract country names\n",
    "countries_rdd = users_rdd.map(lambda row: row.country)\n",
    "unique_countries = countries_rdd.distinct().collect()\n",
    "\n",
    "print(f'\\n🌍 Unique Countries: {unique_countries}')\n",
    "\n",
    "# Filter operation: users over 30\n",
    "mature_users_rdd = users_rdd.filter(lambda row: row.age > 30)\n",
    "mature_count = mature_users_rdd.count()\n",
    "\n",
    "print(f'\\n👥 Users over 30: {mature_count}')\n",
    "\n",
    "# Reduce operation: sum of all ages\n",
    "total_age = users_rdd.map(lambda row: row.age).reduce(lambda a, b: a + b)\n",
    "avg_age = total_age / users_rdd.count()\n",
    "\n",
    "print(f'\\n📈 Average Age (via RDD): {avg_age:.1f} years')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Spark SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Register DataFrames as temporary views\n",
    "print('🗄️  Creating Temporary Views for Spark SQL:')\n",
    "\n",
    "users_df.createOrReplaceTempView(\"users\")\n",
    "transactions_df.createOrReplaceTempView(\"transactions\")\n",
    "\n",
    "print('✅ Temporary views created: users, transactions')\n",
    "\n",
    "# SQL Query 1: User demographics\n",
    "print('\\n📊 SQL Query: User Demographics by Country')\n",
    "demographics_sql = \"\"\"\n",
    "SELECT \n",
    "    country,\n",
    "    COUNT(*) as user_count,\n",
    "    AVG(age) as avg_age,\n",
    "    MIN(age) as min_age,\n",
    "    MAX(age) as max_age\n",
    "FROM users \n",
    "GROUP BY country \n",
    "ORDER BY user_count DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(demographics_sql).show()\n",
    "\n",
    "# SQL Query 2: User transaction summary\n",
    "print('\\n💳 SQL Query: User Transaction Summary')\n",
    "transaction_summary_sql = \"\"\"\n",
    "SELECT \n",
    "    u.name,\n",
    "    u.country,\n",
    "    COUNT(t.transaction_id) as total_transactions,\n",
    "    SUM(t.amount) as total_spent,\n",
    "    AVG(t.amount) as avg_transaction\n",
    "FROM users u\n",
    "JOIN transactions t ON u.user_id = t.user_id\n",
    "WHERE t.status = 'completed'\n",
    "GROUP BY u.user_id, u.name, u.country\n",
    "ORDER BY total_spent DESC\n",
    "\"\"\"\n",
    "\n",
    "spark.sql(transaction_summary_sql).show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Save Results to HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save processed data to HDFS\n",
    "print('💾 Saving Results to HDFS:')\n",
    "\n",
    "try:\n",
    "    # Save user spending summary as Parquet\n",
    "    output_path = \"hdfs://namenode:9000/user/demo/output/user_spending_summary\"\n",
    "    \n",
    "    user_spending.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(output_path)\n",
    "    \n",
    "    print(f'✅ User spending summary saved to: {output_path}')\n",
    "    \n",
    "    # Save country demographics as JSON\n",
    "    demographics_path = \"hdfs://namenode:9000/user/demo/output/country_demographics\"\n",
    "    \n",
    "    country_demographics = spark.sql(demographics_sql)\n",
    "    country_demographics.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .json(demographics_path)\n",
    "    \n",
    "    print(f'✅ Country demographics saved to: {demographics_path}')\n",
    "    \n",
    "    # Save as CSV too\n",
    "    csv_path = \"hdfs://namenode:9000/user/demo/output/user_spending_csv\"\n",
    "    \n",
    "    user_spending.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(csv_path)\n",
    "    \n",
    "    print(f'✅ User spending CSV saved to: {csv_path}')\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f'❌ Error saving to HDFS: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Performance Monitoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Monitor Spark application performance\n",
    "print('📊 Spark Application Performance:')\n",
    "\n",
    "# Get application info\n",
    "sc = spark.sparkContext\n",
    "print(f'📱 Application Name: {sc.appName}')\n",
    "print(f'🆔 Application ID: {sc.applicationId}')\n",
    "print(f'🌐 Master: {sc.master}')\n",
    "print(f'⚙️  Default Parallelism: {sc.defaultParallelism}')\n",
    "\n",
    "# Show execution plan for a complex query\n",
    "print('\\n🔍 Query Execution Plan:')\n",
    "complex_query = user_spending.filter(col(\"total_spent\") > 100)\n",
    "complex_query.explain(True)\n",
    "\n",
    "print('\\n💡 Access Spark UI at: http://localhost:4040')\n",
    "print('💡 Access Spark Master UI at: http://localhost:8080')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10. Cleanup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stop Spark session (optional - typically done at the end)\n",
    "print('🧹 Spark Session Cleanup:')\n",
    "print('💡 Spark session will remain active for other notebooks')\n",
    "print('💡 To stop the session, uncomment the line below:')\n",
    "# spark.stop()\n",
    "\n",
    "print('\\n📊 Session still active - you can continue with other operations')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 🎯 Summary\n",
    "\n",
    "In this notebook, you learned:\n",
    "\n",
    "1. **Spark Session**: How to create and configure a Spark session with Hadoop integration\n",
    "2. **DataFrames**: Reading, transforming, and analyzing data with Spark DataFrames\n",
    "3. **JSON Processing**: Working with JSON data and schema inference\n",
    "4. **Data Joins**: Combining multiple datasets using DataFrame joins\n",
    "5. **RDD Operations**: Low-level data processing with Resilient Distributed Datasets\n",
    "6. **Spark SQL**: Using SQL syntax for data analysis\n",
    "7. **Data Persistence**: Saving results to HDFS in various formats\n",
    "8. **Performance Monitoring**: Understanding Spark execution and optimization\n",
    "\n",
    "### Key Spark Concepts Covered\n",
    "- **Transformations**: `select()`, `filter()`, `groupBy()`, `join()`, `withColumn()`\n",
    "- **Actions**: `show()`, `count()`, `collect()`, `write()`\n",
    "- **SQL Functions**: `avg()`, `sum()`, `min()`, `max()`, `when()`, `split()`\n",
    "- **File Formats**: CSV, JSON, Parquet\n",
    "\n",
    "### Next Steps\n",
    "- Explore the **03-hive-sql.ipynb** notebook to learn Hive integration\n",
    "- Check out the Spark Master UI at http://localhost:8080\n",
    "- Browse the Spark Application UI at http://localhost:4040\n",
    "\n",
    "### 🔗 Useful Links\n",
    "- **Spark Master UI**: http://localhost:8080\n",
    "- **Spark Application UI**: http://localhost:4040\n",
    "- **Spark Documentation**: https://spark.apache.org/docs/latest/\n",
    "- **PySpark API**: https://spark.apache.org/docs/latest/api/python/"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}