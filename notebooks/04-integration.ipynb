{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Full Stack Big Data Integration\n",
    "\n",
    "This notebook demonstrates the complete integration of Hadoop, Spark, and Hive working together.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import all required libraries\n",
    "import findspark\n",
    "findspark.init()\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import json\n",
    "import re\n",
    "\n",
    "print(\"All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive Spark session\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"Full-Stack-BigData-Integration\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .config(\"spark.sql.adaptive.coalescePartitions.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "print(f\"Spark Version: {spark.version}\")\n",
    "print(f\"Master: {spark.sparkContext.master}\")\n",
    "print(f\"Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"Web UI: {spark.sparkContext.uiWebUrl}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load sample data from the mounted data directory\n",
    "print(\"Loading sample datasets...\")\n",
    "\n",
    "# Load users CSV\n",
    "users_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/home/jovyan/data/samples/users.csv\")\n",
    "\n",
    "print(\"Users data:\")\n",
    "users_df.show(5)\n",
    "\n",
    "# Load transactions JSON\n",
    "transactions_df = spark.read \\\n",
    "    .option(\"multiline\", \"true\") \\\n",
    "    .json(\"/home/jovyan/data/samples/transactions.json\")\n",
    "\n",
    "print(\"\\nTransactions data:\")\n",
    "transactions_df.show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process log data\n",
    "print(\"Processing log data...\")\n",
    "\n",
    "# Read log file as text\n",
    "log_rdd = spark.sparkContext.textFile(\"/home/jovyan/data/samples/logs.txt\")\n",
    "\n",
    "# Parse log entries using regex\n",
    "log_pattern = r'(\\d{4}-\\d{2}-\\d{2} \\d{2}:\\d{2}:\\d{2}) (\\w+)\\s+\\[([^\\]]+)\\] (.*)'\n",
    "\n",
    "def parse_log_line(line):\n",
    "    match = re.match(log_pattern, line)\n",
    "    if match:\n",
    "        return (match.group(1), match.group(2), match.group(3), match.group(4))\n",
    "    return None\n",
    "\n",
    "parsed_logs = log_rdd.map(parse_log_line).filter(lambda x: x is not None)\n",
    "\n",
    "# Convert to DataFrame\n",
    "log_schema = StructType([\n",
    "    StructField(\"timestamp\", StringType(), True),\n",
    "    StructField(\"level\", StringType(), True),\n",
    "    StructField(\"thread\", StringType(), True),\n",
    "    StructField(\"message\", StringType(), True)\n",
    "])\n",
    "\n",
    "logs_df = spark.createDataFrame(parsed_logs, log_schema)\n",
    "print(\"Parsed log data:\")\n",
    "logs_df.show(5, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save all data to HDFS\n",
    "print(\"Saving data to HDFS...\")\n",
    "\n",
    "# Save users to HDFS\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/data/users\")\n",
    "\n",
    "# Save transactions to HDFS (partitioned by category)\n",
    "transactions_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"category\") \\\n",
    "    .parquet(\"hdfs://namenode:9000/user/data/transactions\")\n",
    "\n",
    "# Save logs to HDFS (partitioned by level)\n",
    "logs_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .partitionBy(\"level\") \\\n",
    "    .parquet(\"hdfs://namenode:9000/user/data/logs\")\n",
    "\n",
    "print(\"All data saved to HDFS successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create Hive database and tables\n",
    "print(\"Creating Hive database and tables...\")\n",
    "\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS analytics\")\n",
    "spark.sql(\"USE analytics\")\n",
    "\n",
    "# Create users table\n",
    "users_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"users\")\n",
    "\n",
    "# Create transactions table\n",
    "transactions_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"transactions\")\n",
    "\n",
    "# Create logs table\n",
    "logs_df.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .saveAsTable(\"system_logs\")\n",
    "\n",
    "print(\"Hive tables created!\")\n",
    "spark.sql(\"SHOW TABLES\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comprehensive analytics using SQL\n",
    "print(\"Running comprehensive analytics...\")\n",
    "\n",
    "# User transaction analysis\n",
    "user_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.name,\n",
    "        u.city,\n",
    "        u.salary,\n",
    "        COUNT(t.transaction_id) as transaction_count,\n",
    "        SUM(t.amount) as total_spent,\n",
    "        AVG(t.amount) as avg_transaction,\n",
    "        MAX(t.amount) as max_transaction,\n",
    "        COLLECT_LIST(t.category) as categories\n",
    "    FROM users u\n",
    "    LEFT JOIN transactions t ON CONCAT('U', LPAD(CAST(ROW_NUMBER() OVER (ORDER BY u.name) AS STRING), 3, '0')) = t.user_id\n",
    "    GROUP BY u.name, u.city, u.salary\n",
    "    ORDER BY total_spent DESC NULLS LAST\n",
    "\"\"\")\n",
    "\n",
    "print(\"User transaction analysis:\")\n",
    "user_analysis.show(10, truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Category spending analysis\n",
    "category_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount) as total_amount,\n",
    "        AVG(amount) as avg_amount,\n",
    "        MIN(amount) as min_amount,\n",
    "        MAX(amount) as max_amount,\n",
    "        STDDEV(amount) as amount_stddev\n",
    "    FROM transactions\n",
    "    GROUP BY category\n",
    "    ORDER BY total_amount DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"Category spending analysis:\")\n",
    "category_analysis.show()\n",
    "\n",
    "# Convert to Pandas for visualization\n",
    "category_pandas = category_analysis.toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# System logs analysis\n",
    "logs_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        level,\n",
    "        thread,\n",
    "        COUNT(*) as log_count,\n",
    "        COUNT(DISTINCT DATE(timestamp)) as active_days\n",
    "    FROM system_logs\n",
    "    GROUP BY level, thread\n",
    "    ORDER BY log_count DESC\n",
    "\"\"\")\n",
    "\n",
    "print(\"System logs analysis:\")\n",
    "logs_analysis.show()\n",
    "\n",
    "# Error analysis\n",
    "error_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        timestamp,\n",
    "        thread,\n",
    "        message\n",
    "    FROM system_logs\n",
    "    WHERE level = 'ERROR'\n",
    "    ORDER BY timestamp\n",
    "\"\"\")\n",
    "\n",
    "print(\"\\nError logs:\")\n",
    "error_analysis.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced analytics with window functions\n",
    "print(\"Advanced analytics with window functions...\")\n",
    "\n",
    "daily_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        DATE(timestamp) as transaction_date,\n",
    "        category,\n",
    "        SUM(amount) as daily_total,\n",
    "        COUNT(*) as daily_count,\n",
    "        AVG(amount) as daily_avg,\n",
    "        SUM(SUM(amount)) OVER (\n",
    "            PARTITION BY category \n",
    "            ORDER BY DATE(timestamp) \n",
    "            ROWS UNBOUNDED PRECEDING\n",
    "        ) as running_total\n",
    "    FROM transactions\n",
    "    GROUP BY DATE(timestamp), category\n",
    "    ORDER BY transaction_date, category\n",
    "\"\"\")\n",
    "\n",
    "print(\"Daily transaction trends:\")\n",
    "daily_trends.show(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data quality checks\n",
    "print(\"Data quality assessment...\")\n",
    "\n",
    "# Check for data completeness\n",
    "quality_check = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'users' as table_name,\n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN name IS NULL THEN 1 ELSE 0 END) as null_names,\n",
    "        SUM(CASE WHEN salary IS NULL THEN 1 ELSE 0 END) as null_salaries\n",
    "    FROM users\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'transactions' as table_name,\n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN user_id IS NULL THEN 1 ELSE 0 END) as null_user_ids,\n",
    "        SUM(CASE WHEN amount IS NULL OR amount <= 0 THEN 1 ELSE 0 END) as invalid_amounts\n",
    "    FROM transactions\n",
    "    \n",
    "    UNION ALL\n",
    "    \n",
    "    SELECT \n",
    "        'system_logs' as table_name,\n",
    "        COUNT(*) as total_records,\n",
    "        SUM(CASE WHEN timestamp IS NULL THEN 1 ELSE 0 END) as null_timestamps,\n",
    "        SUM(CASE WHEN level IS NULL THEN 1 ELSE 0 END) as null_levels\n",
    "    FROM system_logs\n",
    "\"\"\")\n",
    "\n",
    "print(\"Data quality summary:\")\n",
    "quality_check.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive visualizations\n",
    "plt.figure(figsize=(15, 10))\n",
    "\n",
    "# Subplot 1: Category spending\n",
    "plt.subplot(2, 3, 1)\n",
    "plt.bar(category_pandas['category'], category_pandas['total_amount'])\n",
    "plt.title('Total Spending by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Total Amount ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Subplot 2: Transaction count by category\n",
    "plt.subplot(2, 3, 2)\n",
    "plt.pie(category_pandas['transaction_count'], labels=category_pandas['category'], autopct='%1.1f%%')\n",
    "plt.title('Transaction Distribution by Category')\n",
    "\n",
    "# Subplot 3: Average transaction amount\n",
    "plt.subplot(2, 3, 3)\n",
    "plt.bar(category_pandas['category'], category_pandas['avg_amount'])\n",
    "plt.title('Average Transaction Amount by Category')\n",
    "plt.xlabel('Category')\n",
    "plt.ylabel('Average Amount ($)')\n",
    "plt.xticks(rotation=45)\n",
    "\n",
    "# Get log level data for visualization\n",
    "log_level_data = spark.sql(\"SELECT level, COUNT(*) as count FROM system_logs GROUP BY level\").toPandas()\n",
    "\n",
    "# Subplot 4: Log levels distribution\n",
    "plt.subplot(2, 3, 4)\n",
    "plt.bar(log_level_data['level'], log_level_data['count'])\n",
    "plt.title('Log Entries by Level')\n",
    "plt.xlabel('Log Level')\n",
    "plt.ylabel('Count')\n",
    "\n",
    "# Subplot 5: User salary distribution\n",
    "salary_data = spark.sql(\"SELECT salary FROM users\").toPandas()\n",
    "plt.subplot(2, 3, 5)\n",
    "plt.hist(salary_data['salary'], bins=8, alpha=0.7)\n",
    "plt.title('User Salary Distribution')\n",
    "plt.xlabel('Salary ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "# Subplot 6: Transaction amount distribution\n",
    "amount_data = spark.sql(\"SELECT amount FROM transactions\").toPandas()\n",
    "plt.subplot(2, 3, 6)\n",
    "plt.hist(amount_data['amount'], bins=10, alpha=0.7, color='green')\n",
    "plt.title('Transaction Amount Distribution')\n",
    "plt.xlabel('Amount ($)')\n",
    "plt.ylabel('Frequency')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export final results to HDFS\n",
    "print(\"Exporting final analysis results...\")\n",
    "\n",
    "# Export user analysis\n",
    "user_analysis.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/analytics/user_analysis\")\n",
    "\n",
    "# Export category analysis\n",
    "category_analysis.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/analytics/category_analysis\")\n",
    "\n",
    "# Export daily trends\n",
    "daily_trends.write \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .csv(\"hdfs://namenode:9000/user/analytics/daily_trends\")\n",
    "\n",
    "print(\"All analysis results exported to HDFS!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance summary\n",
    "print(\"=\" * 60)\n",
    "print(\"BIG DATA ENVIRONMENT INTEGRATION SUMMARY\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"✅ Spark Version: {spark.version}\")\n",
    "print(f\"✅ Application ID: {spark.sparkContext.applicationId}\")\n",
    "print(f\"✅ Master: {spark.sparkContext.master}\")\n",
    "print(f\"✅ Default Parallelism: {spark.sparkContext.defaultParallelism}\")\n",
    "print(f\"✅ Web UI: {spark.sparkContext.uiWebUrl}\")\n",
    "print()\n",
    "print(\"Data Processing Summary:\")\n",
    "print(f\"✅ Users processed: {users_df.count()}\")\n",
    "print(f\"✅ Transactions processed: {transactions_df.count()}\")\n",
    "print(f\"✅ Log entries processed: {logs_df.count()}\")\n",
    "print()\n",
    "print(\"Storage Summary:\")\n",
    "print(\"✅ Data saved to HDFS in multiple formats (CSV, Parquet)\")\n",
    "print(\"✅ Hive tables created for structured queries\")\n",
    "print(\"✅ Data partitioned for optimized queries\")\n",
    "print()\n",
    "print(\"Analytics Summary:\")\n",
    "print(\"✅ User transaction analysis completed\")\n",
    "print(\"✅ Category spending analysis completed\")\n",
    "print(\"✅ System logs analysis completed\")\n",
    "print(\"✅ Data quality assessment completed\")\n",
    "print(\"✅ Visualizations generated\")\n",
    "print()\n",
    "print(\"Integration Status:\")\n",
    "print(\"✅ Hadoop HDFS: Working\")\n",
    "print(\"✅ Spark Processing: Working\")\n",
    "print(\"✅ Hive Metastore: Working\")\n",
    "print(\"✅ Jupyter Environment: Working\")\n",
    "print(\"=\" * 60)\n",
    "print(\"🎉 FULL STACK BIG DATA INTEGRATION SUCCESSFUL! 🎉\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean up\n",
    "spark.stop()\n",
    "print(\"Spark session stopped. Integration demonstration complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}