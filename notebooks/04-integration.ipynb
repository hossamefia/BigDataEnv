{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üåü Full Stack Big Data Integration\n",
    "\n",
    "This notebook demonstrates the complete integration of all Big Data components in our environment.\n",
    "\n",
    "## Learning Objectives\n",
    "- Orchestrate HDFS, Spark, and Hive together\n",
    "- Build end-to-end data pipelines\n",
    "- Perform comprehensive data analytics\n",
    "- Demonstrate production-ready workflows\n",
    "- Monitor and optimize performance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Environment Initialization and Health Check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *\n",
    "from hdfs import InsecureClient\n",
    "import pandas as pd\n",
    "import json\n",
    "import time\n",
    "\n",
    "print('üöÄ Big Data Environment - Full Stack Integration')\n",
    "print('=' * 65)\n",
    "\n",
    "# Initialize Spark session with full configuration\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"BigDataEnv-FullIntegration\") \\\n",
    "    .master(\"spark://spark-master:7077\") \\\n",
    "    .config(\"spark.sql.warehouse.dir\", \"hdfs://namenode:9000/user/hive/warehouse\") \\\n",
    "    .config(\"spark.hadoop.fs.defaultFS\", \"hdfs://namenode:9000\") \\\n",
    "    .config(\"spark.sql.catalogImplementation\", \"hive\") \\\n",
    "    .config(\"hive.metastore.uris\", \"thrift://hive-metastore:9083\") \\\n",
    "    .config(\"spark.serializer\", \"org.apache.spark.serializer.KryoSerializer\") \\\n",
    "    .config(\"spark.sql.adaptive.enabled\", \"true\") \\\n",
    "    .enableHiveSupport() \\\n",
    "    .getOrCreate()\n",
    "\n",
    "# Initialize HDFS client\n",
    "try:\n",
    "    hdfs_client = InsecureClient('http://namenode:9870', user='root')\nexcept:\n",
    "    hdfs_client = None\n",
    "\n",
    "print('‚úÖ Spark Session initialized with full Hadoop/Hive integration')\n",
    "print(f'üåê Master: {spark.sparkContext.master}')\n",
    "print(f'üì± Application: {spark.sparkContext.applicationId}')\n",
    "print(f'‚öôÔ∏è  Parallelism: {spark.sparkContext.defaultParallelism}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Comprehensive Data Processing Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('üèóÔ∏è  Building Comprehensive Data Pipeline:')\n",
    "\n",
    "# Create analytics database\n",
    "spark.sql(\"CREATE DATABASE IF NOT EXISTS bigdata_analytics\")\n",
    "spark.sql(\"USE bigdata_analytics\")\n",
    "print('‚úÖ Analytics database ready')\n",
    "\n",
    "# Enhanced users processing with comprehensive transformations\n",
    "print('\\nüë• Processing Enhanced Users Data:')\n",
    "try:\n",
    "    users_df = spark.read \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .option(\"inferSchema\", \"true\") \\\n",
    "        .csv(\"hdfs://namenode:9000/user/demo/input/users.csv\")\n",
    "    \n",
    "    users_enhanced = users_df \\\n",
    "        .withColumn(\"email_domain\", split(col(\"email\"), \"@\").getItem(1)) \\\n",
    "        .withColumn(\"age_group\", \n",
    "                   when(col(\"age\") < 25, \"Young\")\n",
    "                   .when(col(\"age\") < 35, \"Adult\")\n",
    "                   .when(col(\"age\") < 50, \"Mature\")\n",
    "                   .otherwise(\"Senior\")) \\\n",
    "        .withColumn(\"continent\",\n",
    "                   when(col(\"country\").isin([\"USA\", \"Canada\"]), \"North America\")\n",
    "                   .when(col(\"country\").isin([\"UK\", \"Germany\", \"France\", \"Spain\", \"Italy\"]), \"Europe\")\n",
    "                   .when(col(\"country\").isin([\"Australia\"]), \"Oceania\")\n",
    "                   .when(col(\"country\").isin([\"Japan\", \"South Korea\"]), \"Asia\")\n",
    "                   .otherwise(\"Other\")) \\\n",
    "        .withColumn(\"load_timestamp\", current_timestamp())\n",
    "    \n",
    "    print(f'‚úÖ Users enhanced: {users_enhanced.count()} records')\n",
    "    users_enhanced.cache()\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Using sample data: {e}')\n",
    "    users_enhanced = spark.createDataFrame([\n",
    "        (1, \"John Smith\", \"john@email.com\", 28, \"New York\", \"USA\", \"email.com\", \"Adult\", \"North America\"),\n",
    "        (2, \"Emma Johnson\", \"emma@email.com\", 34, \"London\", \"UK\", \"email.com\", \"Adult\", \"Europe\"),\n",
    "        (3, \"Michael Chen\", \"michael@email.com\", 22, \"Toronto\", \"Canada\", \"email.com\", \"Young\", \"North America\")\n",
    "    ], [\"user_id\", \"name\", \"email\", \"age\", \"city\", \"country\", \"email_domain\", \"age_group\", \"continent\"])\n",
    "    users_enhanced = users_enhanced.withColumn(\"load_timestamp\", current_timestamp())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enhanced transactions processing\n",
    "print('üí≥ Processing Enhanced Transactions Data:')\n",
    "try:\n",
    "    transactions_df = spark.read \\\n",
    "        .option(\"multiline\", \"true\") \\\n",
    "        .json(\"hdfs://namenode:9000/user/demo/input/transactions.json\")\n",
    "    \n",
    "    transactions_enhanced = transactions_df \\\n",
    "        .withColumn(\"transaction_date\", to_date(col(\"timestamp\"))) \\\n",
    "        .withColumn(\"amount_usd\",\n",
    "                   when(col(\"currency\") == \"USD\", col(\"amount\"))\n",
    "                   .when(col(\"currency\") == \"EUR\", col(\"amount\") * 1.1)\n",
    "                   .when(col(\"currency\") == \"GBP\", col(\"amount\") * 1.25)\n",
    "                   .otherwise(col(\"amount\"))) \\\n",
    "        .withColumn(\"transaction_size\",\n",
    "                   when(col(\"amount\") < 50, \"Small\")\n",
    "                   .when(col(\"amount\") < 200, \"Medium\") \n",
    "                   .otherwise(\"Large\")) \\\n",
    "        .withColumn(\"load_timestamp\", current_timestamp())\n",
    "    \n",
    "    print(f'‚úÖ Transactions enhanced: {transactions_enhanced.count()} records')\n",
    "    transactions_enhanced.cache()\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Using sample data: {e}')\n",
    "    transactions_enhanced = spark.createDataFrame([\n",
    "        (\"TXN001\", 1, 150.50, \"USD\", \"Amazon\", \"Electronics\", \"completed\", 150.50, \"Medium\"),\n",
    "        (\"TXN002\", 2, 89.99, \"GBP\", \"Tesco\", \"Groceries\", \"completed\", 112.49, \"Medium\"),\n",
    "        (\"TXN003\", 3, 299.00, \"CAD\", \"Best Buy\", \"Electronics\", \"pending\", 224.25, \"Large\")\n",
    "    ], [\"transaction_id\", \"user_id\", \"amount\", \"currency\", \"merchant\", \"category\", \"status\", \"amount_usd\", \"transaction_size\"])\n",
    "    transactions_enhanced = transactions_enhanced.withColumn(\"load_timestamp\", current_timestamp())\n",
    "\n",
    "print('\\nüîç Sample Enhanced Data:')\n",
    "users_enhanced.select(\"user_id\", \"name\", \"age_group\", \"continent\").show(3)\n",
    "transactions_enhanced.select(\"transaction_id\", \"user_id\", \"amount_usd\", \"transaction_size\").show(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Advanced Analytics and Business Intelligence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create comprehensive analytics views\n",
    "print('üìä Creating Analytics Views:')\n",
    "\n",
    "# Register as temporary views for SQL analysis\n",
    "users_enhanced.createOrReplaceTempView(\"users_enhanced\")\n",
    "transactions_enhanced.createOrReplaceTempView(\"transactions_enhanced\")\n",
    "\n",
    "# 1. Customer Segmentation Analysis\n",
    "print('\\nüéØ Customer Segmentation Analysis:')\n",
    "customer_segments = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.age_group,\n",
    "        u.continent,\n",
    "        COUNT(DISTINCT u.user_id) as customer_count,\n",
    "        COUNT(t.transaction_id) as total_transactions,\n",
    "        SUM(t.amount_usd) as total_revenue,\n",
    "        AVG(t.amount_usd) as avg_transaction_value,\n",
    "        SUM(t.amount_usd) / COUNT(DISTINCT u.user_id) as revenue_per_customer\n",
    "    FROM users_enhanced u\n",
    "    LEFT JOIN transactions_enhanced t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.age_group, u.continent\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "customer_segments.show()\n",
    "\n",
    "# 2. Product Category Performance\n",
    "print('\\nüì¶ Product Category Performance:')\n",
    "category_performance = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transaction_count,\n",
    "        SUM(amount_usd) as total_revenue,\n",
    "        AVG(amount_usd) as avg_amount,\n",
    "        COUNT(CASE WHEN status = 'completed' THEN 1 END) * 100.0 / COUNT(*) as success_rate\n",
    "    FROM transactions_enhanced\n",
    "    GROUP BY category\n",
    "    ORDER BY total_revenue DESC\n",
    "\"\"\")\n",
    "category_performance.show()\n",
    "\n",
    "# 3. Geographic Analysis\n",
    "print('\\nüåç Geographic Revenue Analysis:')\n",
    "geographic_analysis = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.continent,\n",
    "        u.country,\n",
    "        COUNT(DISTINCT u.user_id) as customers,\n",
    "        SUM(t.amount_usd) as revenue,\n",
    "        AVG(t.amount_usd) as avg_transaction\n",
    "    FROM users_enhanced u\n",
    "    JOIN transactions_enhanced t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.continent, u.country\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "geographic_analysis.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Data Warehouse Implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create data warehouse tables\n",
    "print('üèõÔ∏è  Creating Data Warehouse:')\n",
    "\n",
    "# Save dimension tables\n",
    "print('\\nüìã Creating Dimension Tables:')\n",
    "try:\n",
    "    users_enhanced.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"dim_users\")\n",
    "    print('‚úÖ dim_users created')\n",
    "    \n",
    "    transactions_enhanced.write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .saveAsTable(\"fact_transactions\")\n",
    "    print('‚úÖ fact_transactions created')\n",
    "\nexcept Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Table creation issue: {e}')\n",
    "\n",
    "# Create summary tables\n",
    "print('\\nüìä Creating Summary Tables:')\n",
    "try:\n",
    "    # User summary table\n",
    "    user_summary = spark.sql(\"\"\"\n",
    "        SELECT \n",
    "            u.user_id,\n",
    "            u.name,\n",
    "            u.age_group,\n",
    "            u.continent,\n",
    "            COUNT(t.transaction_id) as total_transactions,\n",
    "            COALESCE(SUM(CASE WHEN t.status = 'completed' THEN t.amount_usd END), 0) as total_spent,\n",
    "            COALESCE(AVG(CASE WHEN t.status = 'completed' THEN t.amount_usd END), 0) as avg_transaction\n",
    "        FROM users_enhanced u\n",
    "        LEFT JOIN transactions_enhanced t ON u.user_id = t.user_id\n",
    "        GROUP BY u.user_id, u.name, u.age_group, u.continent\n",
    "    \"\"\")\n",
    "    \n",
    "    user_summary.createOrReplaceTempView(\"user_summary\")\n",
    "    print('‚úÖ user_summary view created')\n",
    "    \n",
    "    print('\\nüë• User Summary Sample:')\n",
    "    user_summary.show(5)\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ùå Summary creation error: {e}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Advanced SQL Analytics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Advanced SQL analytics with window functions\n",
    "print('üî¨ Advanced SQL Analytics:')\n",
    "\n",
    "# 1. Customer Ranking Analysis\n",
    "print('\\nüèÜ Top Customers by Spending:')\n",
    "top_customers = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        name,\n",
    "        continent,\n",
    "        total_spent,\n",
    "        total_transactions,\n",
    "        RANK() OVER (ORDER BY total_spent DESC) as spending_rank,\n",
    "        RANK() OVER (PARTITION BY continent ORDER BY total_spent DESC) as continent_rank\n",
    "    FROM user_summary\n",
    "    WHERE total_spent > 0\n",
    "    ORDER BY total_spent DESC\n",
    "\"\"\")\n",
    "top_customers.show(10)\n",
    "\n",
    "# 2. Category Trends Analysis\n",
    "print('\\nüìà Category Performance with Trends:')\n",
    "category_trends = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        category,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(amount_usd) as revenue,\n",
    "        AVG(amount_usd) as avg_amount,\n",
    "        SUM(amount_usd) / SUM(SUM(amount_usd)) OVER () * 100 as revenue_percentage\n",
    "    FROM transactions_enhanced\n",
    "    WHERE status = 'completed'\n",
    "    GROUP BY category\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "category_trends.show()\n",
    "\n",
    "# 3. Executive Summary Dashboard\n",
    "print('\\nüìã Executive Summary:')\n",
    "executive_summary = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        'Total Customers' as metric,\n",
    "        CAST(COUNT(DISTINCT user_id) AS STRING) as value\n",
    "    FROM user_summary\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Total Revenue (USD)' as metric,\n",
    "        CAST(ROUND(SUM(total_spent), 2) AS STRING) as value\n",
    "    FROM user_summary\n",
    "    UNION ALL\n",
    "    SELECT \n",
    "        'Avg Customer Value' as metric,\n",
    "        CAST(ROUND(AVG(total_spent), 2) AS STRING) as value\n",
    "    FROM user_summary\n",
    "    WHERE total_spent > 0\n",
    "\"\"\")\n",
    "executive_summary.show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Data Export and Integration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export processed data\n",
    "print('üì§ Exporting Processed Data:')\n",
    "\n",
    "try:\n",
    "    # Export customer segments\n",
    "    print('\\nüíæ Exporting Customer Segments:')\n",
    "    customer_segments.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .parquet(\"hdfs://namenode:9000/data/exports/customer_segments\")\n",
    "    print('‚úÖ Customer segments exported to Parquet')\n",
    "    \n",
    "    # Export user summary\n",
    "    print('\\nüíæ Exporting User Summary:')\n",
    "    user_summary.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .option(\"header\", \"true\") \\\n",
    "        .csv(\"hdfs://namenode:9000/data/exports/user_summary\")\n",
    "    print('‚úÖ User summary exported to CSV')\n",
    "    \n",
    "    # Export executive summary\n",
    "    print('\\nüíæ Exporting Executive Summary:')\n",
    "    executive_summary.coalesce(1) \\\n",
    "        .write \\\n",
    "        .mode(\"overwrite\") \\\n",
    "        .json(\"hdfs://namenode:9000/data/exports/executive_summary\")\n",
    "    print('‚úÖ Executive summary exported to JSON')\n",
    "    \nexcept Exception as e:\n",
    "    print(f'‚ö†Ô∏è  Export may have issues in this environment: {e}')\n",
    "    print('üí° Data is still available in memory for analysis')\n",
    "\n",
    "print('\\nüìÅ Export Summary:')\n",
    "export_info = [\n",
    "    ('Customer Segments', 'Parquet', '/data/exports/customer_segments'),\n",
    "    ('User Summary', 'CSV', '/data/exports/user_summary'),\n",
    "    ('Executive Summary', 'JSON', '/data/exports/executive_summary')\n",
    "]\n",
    "\n",
    "for name, format_type, path in export_info:\n",
    "    print(f'  {name:18}: {format_type:8} ‚Üí {path}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Performance Monitoring and Optimization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance monitoring\n",
    "print('‚ö° Performance Monitoring:')\n",
    "\n",
    "# Spark application metrics\n",
    "sc = spark.sparkContext\n",
    "print(f'\\nüìä Spark Application:')\n",
    "print(f'  Application ID: {sc.applicationId}')\n",
    "print(f'  Master: {sc.master}')\n",
    "print(f'  Cores: {sc.defaultParallelism}')\n",
    "print(f'  Version: {sc.version}')\n",
    "\n",
    "# Query performance test\n",
    "print('\\nüöÄ Query Performance Test:')\n",
    "start_time = time.time()\n",
    "\n",
    "performance_query = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        u.continent,\n",
    "        COUNT(*) as transactions,\n",
    "        SUM(t.amount_usd) as revenue\n",
    "    FROM users_enhanced u\n",
    "    JOIN transactions_enhanced t ON u.user_id = t.user_id\n",
    "    WHERE t.status = 'completed'\n",
    "    GROUP BY u.continent\n",
    "    ORDER BY revenue DESC\n",
    "\"\"\")\n",
    "\n",
    "result = performance_query.collect()\n",
    "end_time = time.time()\n",
    "\n",
    "print(f'  Query time: {end_time - start_time:.3f} seconds')\n",
    "print(f'  Results: {len(result)} rows')\n",
    "\n",
    "# Show performance results\n",
    "print('\\nüìä Performance Query Results:')\n",
    "performance_query.show()\n",
    "\n",
    "# Cache statistics\n",
    "print('\\nüíæ Cache Status:')\n",
    "cache_status = [\n",
    "    ('users_enhanced', users_enhanced.is_cached),\n",
    "    ('transactions_enhanced', transactions_enhanced.is_cached)\n",
    "]\n",
    "\n",
    "for table, cached in cache_status:\n",
    "    status = \"‚úÖ Cached\" if cached else \"‚ùå Not Cached\"\n",
    "    print(f'  {table:20}: {status}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Integration Summary and Access Points"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Final integration summary\n",
    "print('üéØ Big Data Integration Summary:')\n",
    "print('=' * 50)\n",
    "\n",
    "# Component status\n",
    "components = {\n",
    "    'HDFS': '‚úÖ Distributed file system ready',\n",
    "    'Spark': '‚úÖ Distributed computing active', \n",
    "    'Hive': '‚úÖ Data warehouse with PostgreSQL metastore',\n",
    "    'Jupyter': '‚úÖ Interactive development environment',\n",
    "    'Data Pipeline': '‚úÖ ETL processing complete',\n",
    "    'Analytics': '‚úÖ Business intelligence views created',\n",
    "    'Export': '‚úÖ Multi-format data export ready'\n",
    "}\n",
    "\n",
    "print('\\nüöÄ Component Status:')\n",
    "for component, status in components.items():\n",
    "    print(f'  {component:15}: {status}')\n",
    "\n",
    "# Data pipeline stats\n",
    "print('\\nüìä Pipeline Statistics:')\n",
    "try:\n",
    "    user_count = users_enhanced.count()\n",
    "    transaction_count = transactions_enhanced.count()\n",
    "    \n",
    "    stats = {\n",
    "        'Users Processed': f'{user_count:,}',\n",
    "        'Transactions': f'{transaction_count:,}',\n",
    "        'Data Sources': '3 (CSV, JSON, generated)',\n",
    "        'Export Formats': '3 (Parquet, CSV, JSON)',\n",
    "        'Analytics Views': '5+ (segments, performance, geographic)'\n",
    "    }\n",
    "    \n",
    "    for stat, value in stats.items():\n",
    "        print(f'  {stat:18}: {value}')\nexcept Exception as e:\n",
    "    print(f'  Statistics calculation: {e}')\n",
    "\n",
    "print('\\nüåê Access Points:')\n",
    "access_points = {\n",
    "    'Jupyter Lab': 'http://localhost:8888 (token: bigdata123)',\n",
    "    'Spark Master': 'http://localhost:8080',\n",
    "    'Spark App UI': 'http://localhost:4040',\n",
    "    'HDFS NameNode': 'http://localhost:9870',\n",
    "    'YARN ResourceMgr': 'http://localhost:8088',\n",
    "    'HiveServer2': 'http://localhost:10002'\n",
    "}\n",
    "\n",
    "for service, url in access_points.items():\n",
    "    print(f'  {service:15}: {url}')\n",
    "\n",
    "print('\\nüí° Next Steps:')\n",
    "next_steps = [\n",
    "    '1. Explore the web UIs listed above',\n",
    "    '2. Run custom analytics queries',\n",
    "    '3. Add more data sources and processing',\n",
    "    '4. Implement machine learning models',\n",
    "    '5. Set up automated reporting',\n",
    "    '6. Scale with additional data volumes'\n",
    "]\n",
    "\n",
    "for step in next_steps:\n",
    "    print(f'  {step}')\n",
    "\n",
    "print('\\nüéâ Full Stack Big Data Integration Complete!')\n",
    "print('\\nüåü Environment Features:')\n",
    "features = [\n",
    "    '‚Ä¢ Production-ready Big Data stack',\n",
    "    '‚Ä¢ Reliable PostgreSQL metastore',\n",
    "    '‚Ä¢ Comprehensive data processing pipeline',\n",
    "    '‚Ä¢ Advanced analytics and BI capabilities',\n",
    "    '‚Ä¢ Multi-format data export options',\n",
    "    '‚Ä¢ Performance monitoring tools',\n",
    "    '‚Ä¢ Easy Windows automation scripts'\n",
    "]\n",
    "\n",
    "for feature in features:\n",
    "    print(f'  {feature}')\n",
    "\n",
    "print('\\nüöÄ Ready for Big Data Analysis and Development!')\n",
    "print('\\nüìù Use the other notebooks to explore specific components:')\n",
    "print('  ‚Ä¢ 01-hadoop-basics.ipynb - HDFS operations')\n",
    "print('  ‚Ä¢ 02-spark-intro.ipynb - Spark fundamentals')\n",
    "print('  ‚Ä¢ 03-hive-sql.ipynb - Hive SQL operations')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}