# Spark Configuration for Big Data Environment
# 
# This configuration optimizes Spark for integration with Hadoop and Hive
# in a development/testing environment

# Spark Master Configuration
spark.master                    spark://spark-master:7077
spark.app.name                  BigDataEnv-Spark

# Spark SQL and Hive Integration
spark.sql.catalogImplementation    hive
spark.sql.warehouse.dir            hdfs://namenode:9000/user/hive/warehouse
spark.sql.hive.metastore.version   3.1.3
spark.sql.hive.metastore.jars      builtin

# Hadoop Integration
spark.hadoop.fs.defaultFS                  hdfs://namenode:9000
spark.hadoop.yarn.resourcemanager.address  resourcemanager:8032
spark.hadoop.yarn.resourcemanager.hostname resourcemanager

# Event Logging for Spark History Server
spark.eventLog.enabled          true
spark.eventLog.dir              hdfs://namenode:9000/spark-history
spark.history.fs.logDirectory   hdfs://namenode:9000/spark-history

# Serialization (Kryo is faster than Java serialization)
spark.serializer                org.apache.spark.serializer.KryoSerializer
spark.kryo.unsafe               true
spark.kryo.referenceTracking    false

# Memory Configuration
spark.executor.memory           1g
spark.executor.cores            1
spark.driver.memory             512m
spark.driver.cores              1
spark.executor.instances        2

# Dynamic Allocation
spark.dynamicAllocation.enabled            true
spark.dynamicAllocation.minExecutors       1
spark.dynamicAllocation.maxExecutors       4
spark.dynamicAllocation.initialExecutors   2
spark.shuffle.service.enabled              true

# Performance Tuning
spark.sql.adaptive.enabled                 true
spark.sql.adaptive.coalescePartitions.enabled  true
spark.sql.adaptive.skewJoin.enabled        true
spark.sql.cbo.enabled                      true
spark.sql.statistics.histogram.enabled     true

# Compression
spark.sql.parquet.compression.codec        snappy
spark.io.compression.codec                 snappy

# Network and Communication
spark.network.timeout                      800s
spark.executor.heartbeatInterval           60s
spark.sql.broadcastTimeout                 36000

# Logging Configuration
spark.eventLog.compress                    true
spark.sql.execution.arrow.pyspark.enabled  true

# PySpark Configuration
spark.pyspark.python                       python3
spark.pyspark.driver.python                python3

# Security (disabled for development environment)
spark.authenticate                         false
spark.network.crypto.enabled              false
spark.io.encryption.enabled               false

# UI Configuration
spark.ui.enabled                           true
spark.ui.port                              4040
spark.ui.retainedJobs                      100
spark.ui.retainedStages                    100

# Checkpoint Directory
spark.sql.streaming.checkpointLocation     hdfs://namenode:9000/checkpoints

# Additional JARs and Packages
spark.jars.packages                        org.postgresql:postgresql:42.5.0

# Delta Lake Support (if needed)
# spark.sql.extensions                     io.delta.sql.DeltaSparkSessionExtension
# spark.sql.catalog.spark_catalog          org.apache.spark.sql.delta.catalog.DeltaCatalog