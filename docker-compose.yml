version: '3.8'

services:
  # PostgreSQL - Hive Metastore Database
  postgres:
    image: postgres:13
    container_name: bigdata-postgres
    environment:
      POSTGRES_DB: metastore
      POSTGRES_USER: hive
      POSTGRES_PASSWORD: hive123
    volumes:
      - postgres_data:/var/lib/postgresql/data
      - ./configs/hive/init-metastore.sql:/docker-entrypoint-initdb.d/init-metastore.sql:ro
    ports:
      - "5432:5432"
    networks:
      - bigdata-network
    healthcheck:
      test: ["CMD-SHELL", "pg_isready -U hive -d metastore"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Hadoop NameNode - HDFS Master
  namenode:
    image: apache/hadoop:3.3.4
    container_name: bigdata-namenode
    environment:
      CLUSTER_NAME: bigdata-cluster
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      CORE_CONF_hadoop_proxyuser_hue_hosts: "*"
      CORE_CONF_hadoop_proxyuser_hue_groups: "*"
      CORE_CONF_io_compression_codecs: org.apache.hadoop.io.compress.SnappyCodec
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_nameservices: hdfscluster
      HDFS_CONF_dfs_ha_namenodes_hdfscluster: nn1
      HDFS_CONF_dfs_namenode_rpc_address_hdfscluster_nn1: namenode:9000
      HDFS_CONF_dfs_namenode_http_address_hdfscluster_nn1: namenode:9870
      HDFS_CONF_dfs_replication: 1
      HDFS_CONF_dfs_namenode_name_dir: /hadoop/dfs/name
    volumes:
      - namenode_data:/hadoop/dfs/name
      - ./configs/hadoop:/opt/hadoop/etc/hadoop:ro
    ports:
      - "9870:9870"
      - "9000:9000"
    networks:
      - bigdata-network
    command: ["hdfs", "namenode"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9870/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Hadoop DataNode - HDFS Storage
  datanode:
    image: apache/hadoop:3.3.4
    container_name: bigdata-datanode
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      HDFS_CONF_dfs_webhdfs_enabled: "true"
      HDFS_CONF_dfs_permissions_enabled: "false"
      HDFS_CONF_dfs_datanode_data_dir: /hadoop/dfs/data
      HDFS_CONF_dfs_datanode_http_address: 0.0.0.0:9864
      HDFS_CONF_dfs_replication: 1
    volumes:
      - datanode_data:/hadoop/dfs/data
      - ./configs/hadoop:/opt/hadoop/etc/hadoop:ro
    ports:
      - "9864:9864"
    networks:
      - bigdata-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["hdfs", "datanode"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:9864/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # YARN ResourceManager - Cluster Resources
  resourcemanager:
    image: apache/hadoop:3.3.4
    container_name: bigdata-resourcemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      YARN_CONF_yarn_web_proxy_address: resourcemanager:8089
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
      YARN_CONF_yarn_resourcemanager_webapp_address: resourcemanager:8088
      YARN_CONF_yarn_resourcemanager_scheduler_address: resourcemanager:8030
      YARN_CONF_yarn_resourcemanager_address: resourcemanager:8032
      YARN_CONF_yarn_resourcemanager_admin_address: resourcemanager:8033
      YARN_CONF_yarn_resourcemanager_resource_tracker_address: resourcemanager:8031
      YARN_CONF_yarn_nodemanager_remote_app_log_dir: /app-logs
      YARN_CONF_yarn_log_aggregation_enable: "true"
      YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle
    volumes:
      - ./configs/hadoop:/opt/hadoop/etc/hadoop:ro
    ports:
      - "8088:8088"
    networks:
      - bigdata-network
    depends_on:
      namenode:
        condition: service_healthy
    command: ["yarn", "resourcemanager"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8088/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # YARN NodeManager - Node Resources
  nodemanager:
    image: apache/hadoop:3.3.4
    container_name: bigdata-nodemanager
    environment:
      CORE_CONF_fs_defaultFS: hdfs://namenode:9000
      CORE_CONF_hadoop_http_staticuser_user: root
      YARN_CONF_yarn_nodemanager_remote_app_log_dir: /app-logs
      YARN_CONF_yarn_log_aggregation_enable: "true"
      YARN_CONF_yarn_resourcemanager_hostname: resourcemanager
      YARN_CONF_yarn_nodemanager_aux_services: mapreduce_shuffle
      YARN_CONF_yarn_nodemanager_webapp_address: nodemanager:8042
      YARN_CONF_yarn_nodemanager_resource_memory_mb: 2048
      YARN_CONF_yarn_nodemanager_resource_cpu_vcores: 2
      YARN_CONF_yarn_nodemanager_disk_health_checker_max_disk_utilization_per_disk_percentage: 98.5
      YARN_CONF_yarn_nodemanager_remote_app_log_dir_suffix: logs
      YARN_CONF_yarn_nodemanager_log_dirs: /tmp/logs
    volumes:
      - ./configs/hadoop:/opt/hadoop/etc/hadoop:ro
    ports:
      - "8042:8042"
    networks:
      - bigdata-network
    depends_on:
      resourcemanager:
        condition: service_healthy
    command: ["yarn", "nodemanager"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8042/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Hive Metastore Service
  hive-metastore:
    image: apache/hive:3.1.3
    container_name: bigdata-hive-metastore
    environment:
      DB_DRIVER: postgres
      SERVICE_NAME: metastore
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive123"
    volumes:
      - ./configs/hive:/opt/hive/conf:ro
    networks:
      - bigdata-network
    depends_on:
      postgres:
        condition: service_healthy
      namenode:
        condition: service_healthy
    command: ["/opt/hive/bin/hive", "--service", "metastore"]
    healthcheck:
      test: ["CMD-SHELL", "netstat -ln | grep 9083 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # HiveServer2 - SQL Interface
  hiveserver2:
    image: apache/hive:3.1.3
    container_name: bigdata-hiveserver2
    environment:
      HIVE_SERVER2_THRIFT_PORT: 10000
      SERVICE_NAME: hiveserver2
      SERVICE_OPTS: "-Djavax.jdo.option.ConnectionDriverName=org.postgresql.Driver -Djavax.jdo.option.ConnectionURL=jdbc:postgresql://postgres:5432/metastore -Djavax.jdo.option.ConnectionUserName=hive -Djavax.jdo.option.ConnectionPassword=hive123"
    volumes:
      - ./configs/hive:/opt/hive/conf:ro
    ports:
      - "10000:10000"
      - "10002:10002"
    networks:
      - bigdata-network
    depends_on:
      hive-metastore:
        condition: service_healthy
    command: ["/opt/hive/bin/hive", "--service", "hiveserver2"]
    healthcheck:
      test: ["CMD-SHELL", "netstat -ln | grep 10000 || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Spark Master - Cluster Coordinator
  spark-master:
    image: apache/spark:3.4.1-scala2.12-java11-python3-ubuntu
    container_name: bigdata-spark-master
    environment:
      SPARK_MODE: master
      SPARK_MASTER_HOST: spark-master
      SPARK_MASTER_PORT: 7077
      SPARK_MASTER_WEBUI_PORT: 8080
      SPARK_CONF_DIR: /opt/spark/conf
    volumes:
      - ./configs/spark:/opt/spark/conf:ro
      - spark_logs:/opt/spark/logs
    ports:
      - "8080:8080"
      - "7077:7077"
    networks:
      - bigdata-network
    depends_on:
      namenode:
        condition: service_healthy
      resourcemanager:
        condition: service_healthy
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.master.Master"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8080/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Spark Worker - Execution Nodes
  spark-worker:
    image: apache/spark:3.4.1-scala2.12-java11-python3-ubuntu
    container_name: bigdata-spark-worker
    environment:
      SPARK_MODE: worker
      SPARK_MASTER_URL: spark://spark-master:7077
      SPARK_WORKER_WEBUI_PORT: 8081
      SPARK_WORKER_MEMORY: 2g
      SPARK_WORKER_CORES: 2
      SPARK_CONF_DIR: /opt/spark/conf
    volumes:
      - ./configs/spark:/opt/spark/conf:ro
      - spark_logs:/opt/spark/logs
    ports:
      - "8081:8081"
    networks:
      - bigdata-network
    depends_on:
      spark-master:
        condition: service_healthy
    command: ["/opt/spark/bin/spark-class", "org.apache.spark.deploy.worker.Worker", "spark://spark-master:7077"]
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8081/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

  # Jupyter Lab - Interactive Development
  jupyter:
    image: jupyter/pyspark-notebook:spark-3.4.1
    container_name: bigdata-jupyter
    environment:
      JUPYTER_ENABLE_LAB: "yes"
      JUPYTER_TOKEN: bigdata123
      SPARK_MASTER: spark://spark-master:7077
      HADOOP_CONF_DIR: /opt/hadoop/etc/hadoop
      HIVE_CONF_DIR: /opt/hive/conf
    volumes:
      - ./notebooks:/home/jovyan/notebooks
      - ./data:/home/jovyan/data
      - ./configs/jupyter:/home/jovyan/.jupyter:ro
      - ./configs/hadoop:/opt/hadoop/etc/hadoop:ro
      - ./configs/hive:/opt/hive/conf:ro
      - jupyter_home:/home/jovyan
    ports:
      - "8888:8888"
    networks:
      - bigdata-network
    depends_on:
      spark-master:
        condition: service_healthy
      hiveserver2:
        condition: service_healthy
    user: root
    command: start-notebook.sh --NotebookApp.token='bigdata123' --NotebookApp.password='' --NotebookApp.allow_root=True
    healthcheck:
      test: ["CMD-SHELL", "curl -f http://localhost:8888/ || exit 1"]
      interval: 30s
      timeout: 10s
      retries: 5
    restart: unless-stopped

# Named volumes for data persistence
volumes:
  postgres_data:
    driver: local
  namenode_data:
    driver: local
  datanode_data:
    driver: local
  spark_logs:
    driver: local
  jupyter_home:
    driver: local

# Custom network for service communication
networks:
  bigdata-network:
    driver: bridge
    ipam:
      config:
        - subnet: 172.25.0.0/16